{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "722daa86",
   "metadata": {},
   "source": [
    "# Gravity Guy ML Baseline Analysis\n",
    "\n",
    "This notebook analyzes the performance of different baseline agents for the Gravity Guy reinforcement learning environment. We compare a random policy against several heuristic approaches to establish performance benchmarks before implementing machine learning strategies.\n",
    "\n",
    "## Environment Overview\n",
    "\n",
    "The Gravity Guy environment is a 2D platformer where:\n",
    "- The player automatically scrolls right at constant speed\n",
    "- Platforms exist on top and bottom lanes\n",
    "- The player can flip gravity to switch between lanes\n",
    "- Goal: Survive as long as possible by avoiding going out of bounds\n",
    "- Reward: Distance traveled minus small penalty for gravity flips\n",
    "\n",
    "**Observation Space (6D vector):**\n",
    "- `y_norm`: Normalized vertical position [0,1]\n",
    "- `vy_norm`: Normalized vertical velocity [-1,1] \n",
    "- `grav_dir`: Gravity direction (+1 down, -1 up)\n",
    "- `probe1`, `probe2`, `probe3`: Clearance sensors at 60px, 180px, 300px ahead [0,1]\n",
    "\n",
    "**Action Space:** Discrete (0=wait, 1=attempt gravity flip)\n",
    "\n",
    "## Loading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b6cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No runs found for tag=random\n",
      "No runs found for tag=heuristic\n",
      "No runs found for tag=improved\n",
      "No runs found for tag=conservative\n",
      "No runs found for tag=aggressive\n",
      "\n",
      "Datasets loaded:\n",
      "Random: 0 episodes\n",
      "Heuristic: 0 episodes\n",
      "Improved: 0 episodes\n",
      "Conservative: 0 episodes\n",
      "Aggressive: 0 episodes\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import statistics as st\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def load_latest_runs(tag: str):\n",
    "    \"\"\"Load the most recent run data for a given policy tag\"\"\"\n",
    "    paths = sorted(glob.glob(f\"../runs/*_{tag}.jsonl\"))\n",
    "    if not paths:\n",
    "        print(f\"No runs found for tag={tag}\")\n",
    "        return []\n",
    "    \n",
    "    path = paths[-1]  # Most recent\n",
    "    print(f\"Loading {tag}: {path}\")\n",
    "    \n",
    "    episodes = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            episodes.append(json.loads(line.strip()))\n",
    "    \n",
    "    return episodes\n",
    "\n",
    "# Load all baseline agent results\n",
    "random_data = load_latest_runs(\"random\")\n",
    "heuristic_data = load_latest_runs(\"heuristic\") \n",
    "improved_data = load_latest_runs(\"improved\")\n",
    "conservative_data = load_latest_runs(\"conservative\")\n",
    "aggressive_data = load_latest_runs(\"aggressive\")\n",
    "\n",
    "print(f\"\\nDatasets loaded:\")\n",
    "print(f\"Random: {len(random_data)} episodes\")\n",
    "print(f\"Heuristic: {len(heuristic_data)} episodes\") \n",
    "print(f\"Improved: {len(improved_data)} episodes\")\n",
    "print(f\"Conservative: {len(conservative_data)} episodes\")\n",
    "print(f\"Aggressive: {len(aggressive_data)} episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53ae549",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "\n",
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d93571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(episodes, policy_name):\n",
    "    \"\"\"Calculate comprehensive performance statistics\"\"\"\n",
    "    if not episodes:\n",
    "        return None\n",
    "        \n",
    "    distances = [e[\"distance_px\"] for e in episodes]\n",
    "    returns = [e[\"total_return\"] for e in episodes] \n",
    "    flips = [e[\"flips\"] for e in episodes]\n",
    "    times = [e[\"time_s\"] for e in episodes]\n",
    "    \n",
    "    # Termination analysis\n",
    "    out_of_bounds = sum(1 for e in episodes if e.get(\"out_of_bounds\", False))\n",
    "    time_ups = sum(1 for e in episodes if e.get(\"time_up\", False))\n",
    "    \n",
    "    return {\n",
    "        \"Policy\": policy_name,\n",
    "        \"Episodes\": len(episodes),\n",
    "        \"Avg Distance\": np.mean(distances),\n",
    "        \"Std Distance\": np.std(distances),\n",
    "        \"Max Distance\": np.max(distances), \n",
    "        \"Min Distance\": np.min(distances),\n",
    "        \"Avg Return\": np.mean(returns),\n",
    "        \"Avg Flips\": np.mean(flips),\n",
    "        \"Avg Time\": np.mean(times),\n",
    "        \"Success Rate\": time_ups / len(episodes),  # Reached time limit\n",
    "        \"Death Rate\": out_of_bounds / len(episodes),  # Went out of bounds\n",
    "        \"Efficiency\": np.mean(distances) / np.mean(flips) if np.mean(flips) > 0 else np.inf\n",
    "    }\n",
    "\n",
    "# Calculate statistics for all policies\n",
    "policies_data = [\n",
    "    (random_data, \"Random\"),\n",
    "    (heuristic_data, \"Heuristic\"), \n",
    "    (improved_data, \"Improved\"),\n",
    "    (conservative_data, \"Conservative\"),\n",
    "    (aggressive_data, \"Aggressive\")\n",
    "]\n",
    "\n",
    "results = []\n",
    "for data, name in policies_data:\n",
    "    stats = calculate_stats(data, name)\n",
    "    if stats:\n",
    "        results.append(stats)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "df = df.round(2)\n",
    "\n",
    "# Display results table\n",
    "print(\"=== BASELINE AGENT COMPARISON ===\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Calculate improvements over random baseline\n",
    "if len(results) > 0:\n",
    "    baseline_distance = df[df[\"Policy\"] == \"Random\"][\"Avg Distance\"].iloc[0]\n",
    "    df[\"Distance Improvement (%)\"] = ((df[\"Avg Distance\"] - baseline_distance) / baseline_distance * 100).round(1)\n",
    "    \n",
    "    print(f\"\\n=== IMPROVEMENT OVER RANDOM BASELINE ===\")\n",
    "    improvement_df = df[[\"Policy\", \"Avg Distance\", \"Distance Improvement (%)\", \"Avg Flips\"]].copy()\n",
    "    print(improvement_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fe7e9f",
   "metadata": {},
   "source": [
    "### Performance Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf59e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Gravity Guy Baseline Agent Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Extract data for plotting\n",
    "policies = [name for _, name in policies_data if len([d for d, n in policies_data if n == name][0]) > 0]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\n",
    "\n",
    "# 1. Distance Performance Comparison\n",
    "ax1 = axes[0, 0]\n",
    "distances_by_policy = []\n",
    "labels = []\n",
    "\n",
    "for i, (data, name) in enumerate(policies_data):\n",
    "    if data:\n",
    "        distances = [e[\"distance_px\"] for e in data]\n",
    "        distances_by_policy.append(distances)\n",
    "        labels.append(name)\n",
    "\n",
    "box_plot = ax1.boxplot(distances_by_policy, labels=labels, patch_artist=True)\n",
    "for patch, color in zip(box_plot['boxes'], colors[:len(labels)]):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax1.set_title('Distance Distribution by Policy', fontweight='bold')\n",
    "ax1.set_ylabel('Distance (pixels)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Average Performance Bar Chart\n",
    "ax2 = axes[0, 1]\n",
    "if results:\n",
    "    policies_names = df[\"Policy\"].tolist()\n",
    "    avg_distances = df[\"Avg Distance\"].tolist()\n",
    "    \n",
    "    bars = ax2.bar(policies_names, avg_distances, color=colors[:len(policies_names)], alpha=0.8)\n",
    "    ax2.set_title('Average Distance Performance', fontweight='bold')\n",
    "    ax2.set_ylabel('Average Distance (pixels)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, avg_distances):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20, \n",
    "                f'{val:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Flip Efficiency Analysis\n",
    "ax3 = axes[1, 0]\n",
    "if results:\n",
    "    avg_flips = df[\"Avg Flips\"].tolist()\n",
    "    scatter = ax3.scatter(avg_flips, avg_distances, \n",
    "                         c=colors[:len(policies_names)], s=100, alpha=0.8)\n",
    "    \n",
    "    for i, policy in enumerate(policies_names):\n",
    "        ax3.annotate(policy, (avg_flips[i], avg_distances[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontweight='bold')\n",
    "    \n",
    "    ax3.set_title('Distance vs Flip Count Efficiency', fontweight='bold')\n",
    "    ax3.set_xlabel('Average Flips per Episode')\n",
    "    ax3.set_ylabel('Average Distance (pixels)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Episode Length Distribution\n",
    "ax4 = axes[1, 1]\n",
    "episode_times = []\n",
    "time_labels = []\n",
    "\n",
    "for data, name in policies_data:\n",
    "    if data:\n",
    "        times = [e[\"time_s\"] for e in data]\n",
    "        episode_times.append(times)\n",
    "        time_labels.append(name)\n",
    "\n",
    "if episode_times:\n",
    "    time_box_plot = ax4.boxplot(episode_times, labels=time_labels, patch_artist=True)\n",
    "    for patch, color in zip(time_box_plot['boxes'], colors[:len(time_labels)]):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "\n",
    "ax4.set_title('Episode Duration Distribution', fontweight='bold')\n",
    "ax4.set_ylabel('Time (seconds)')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b152c5",
   "metadata": {},
   "source": [
    "### Detailed Episode Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3c677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze individual episodes to understand behavior patterns\n",
    "def analyze_episode_patterns():\n",
    "    print(\"=== EPISODE PATTERN ANALYSIS ===\\n\")\n",
    "    \n",
    "    for data, name in policies_data:\n",
    "        if not data:\n",
    "            continue\n",
    "            \n",
    "        print(f\"--- {name.upper()} POLICY ---\")\n",
    "        \n",
    "        # Success vs failure analysis\n",
    "        successes = [e for e in data if e.get(\"time_up\", False)]  # Hit time limit\n",
    "        failures = [e for e in data if e.get(\"out_of_bounds\", False)]  # Died\n",
    "        \n",
    "        print(f\"Episodes: {len(data)}\")\n",
    "        print(f\"Successes (time limit): {len(successes)}\")\n",
    "        print(f\"Failures (out of bounds): {len(failures)}\")\n",
    "        \n",
    "        if successes:\n",
    "            success_dist = [e[\"distance_px\"] for e in successes]\n",
    "            print(f\"Success distances: {success_dist}\")\n",
    "        \n",
    "        if failures:\n",
    "            failure_dist = [e[\"distance_px\"] for e in failures]\n",
    "            failure_time = [e[\"time_s\"] for e in failures]\n",
    "            print(f\"Failure distances: mean={np.mean(failure_dist):.0f}px, range={min(failure_dist)}-{max(failure_dist)}px\")\n",
    "            print(f\"Failure times: mean={np.mean(failure_time):.1f}s, range={min(failure_time):.1f}-{max(failure_time):.1f}s\")\n",
    "        \n",
    "        # Flip behavior analysis\n",
    "        flips = [e[\"flips\"] for e in data]\n",
    "        distances = [e[\"distance_px\"] for e in data]\n",
    "        flip_efficiency = [d/f if f > 0 else d for d, f in zip(distances, flips)]\n",
    "        \n",
    "        print(f\"Flip efficiency: {np.mean(flip_efficiency):.0f} pixels/flip\")\n",
    "        print(f\"Flip variance: std={np.std(flips):.1f}\")\n",
    "        \n",
    "        # Probe analysis (obstacle detection)\n",
    "        final_probes = [e.get(\"probes\", [1.0, 1.0, 1.0]) for e in data]\n",
    "        if final_probes and len(final_probes[0]) == 3:\n",
    "            avg_p1 = np.mean([p[0] for p in final_probes])\n",
    "            avg_p2 = np.mean([p[1] for p in final_probes])  \n",
    "            avg_p3 = np.mean([p[2] for p in final_probes])\n",
    "            print(f\"Final probe clearances: [{avg_p1:.2f}, {avg_p2:.2f}, {avg_p3:.2f}]\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "analyze_episode_patterns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b95926a",
   "metadata": {},
   "source": [
    "### Learning Curve Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7966a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot episode-by-episode performance to see consistency\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Distance progression\n",
    "for i, (data, name) in enumerate(policies_data):\n",
    "    if data:\n",
    "        episodes = range(len(data))\n",
    "        distances = [e[\"distance_px\"] for e in data]\n",
    "        ax1.plot(episodes, distances, 'o-', label=name, color=colors[i], alpha=0.8, linewidth=2)\n",
    "\n",
    "ax1.set_title('Distance Performance by Episode', fontweight='bold')\n",
    "ax1.set_xlabel('Episode Number')\n",
    "ax1.set_ylabel('Distance (pixels)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Flip count progression\n",
    "for i, (data, name) in enumerate(policies_data):\n",
    "    if data:\n",
    "        episodes = range(len(data))\n",
    "        flips = [e[\"flips\"] for e in data]\n",
    "        ax2.plot(episodes, flips, 's-', label=name, color=colors[i], alpha=0.8, linewidth=2)\n",
    "\n",
    "ax2.set_title('Flip Count by Episode', fontweight='bold')\n",
    "ax2.set_xlabel('Episode Number')\n",
    "ax2.set_ylabel('Number of Flips')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3433f2",
   "metadata": {},
   "source": [
    "## Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6184966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def statistical_comparison():\n",
    "    \"\"\"Perform statistical tests to validate performance differences\"\"\"\n",
    "    print(\"=== STATISTICAL SIGNIFICANCE ANALYSIS ===\\n\")\n",
    "    \n",
    "    if len(random_data) == 0 or len(heuristic_data) == 0:\n",
    "        print(\"Insufficient data for statistical testing\")\n",
    "        return\n",
    "    \n",
    "    # Extract distance data\n",
    "    random_distances = [e[\"distance_px\"] for e in random_data]\n",
    "    heuristic_distances = [e[\"distance_px\"] for e in heuristic_data]\n",
    "    \n",
    "    # Perform t-test\n",
    "    t_stat, p_value = stats.ttest_ind(heuristic_distances, random_distances)\n",
    "    \n",
    "    print(f\"Random vs Heuristic T-Test:\")\n",
    "    print(f\"  Random: mean={np.mean(random_distances):.1f}px, std={np.std(random_distances):.1f}\")\n",
    "    print(f\"  Heuristic: mean={np.mean(heuristic_distances):.1f}px, std={np.std(heuristic_distances):.1f}\")\n",
    "    print(f\"  T-statistic: {t_stat:.3f}\")\n",
    "    print(f\"  P-value: {p_value:.6f}\")\n",
    "    print(f\"  Significant: {'Yes' if p_value < 0.05 else 'No'} (α=0.05)\")\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt(((len(heuristic_distances)-1)*np.std(heuristic_distances)**2 + \n",
    "                         (len(random_distances)-1)*np.std(random_distances)**2) / \n",
    "                        (len(heuristic_distances) + len(random_distances) - 2))\n",
    "    cohens_d = (np.mean(heuristic_distances) - np.mean(random_distances)) / pooled_std\n",
    "    print(f\"  Cohen's d: {cohens_d:.3f} ({'Large' if abs(cohens_d) > 0.8 else 'Medium' if abs(cohens_d) > 0.5 else 'Small'} effect)\")\n",
    "\n",
    "statistical_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2365ac2c",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### 1. Baseline Performance Hierarchy\n",
    "\n",
    "Based on our analysis, the performance ranking is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d3f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final ranking\n",
    "if results:\n",
    "    ranking_df = df[[\"Policy\", \"Avg Distance\", \"Avg Flips\", \"Success Rate\"]].copy()\n",
    "    ranking_df = ranking_df.sort_values(\"Avg Distance\", ascending=False)\n",
    "    \n",
    "    print(\"=== FINAL PERFORMANCE RANKING ===\")\n",
    "    for i, row in ranking_df.iterrows():\n",
    "        rank = ranking_df.index.get_loc(i) + 1\n",
    "        policy = row[\"Policy\"]\n",
    "        dist = row[\"Avg Distance\"]\n",
    "        flips = row[\"Avg Flips\"] \n",
    "        success = row[\"Success Rate\"]\n",
    "        print(f\"{rank}. {policy:>12}: {dist:6.1f}px  ({flips:4.1f} flips, {success:4.1%} success)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d83215",
   "metadata": {},
   "source": [
    "### 2. Behavior Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21de9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== BEHAVIOR PATTERN SUMMARY ===\")\n",
    "\n",
    "# Identify the most efficient strategies\n",
    "if results:\n",
    "    best_policy = df.loc[df[\"Avg Distance\"].idxmax(), \"Policy\"]\n",
    "    most_efficient = df.loc[df[\"Efficiency\"].idxmax(), \"Policy\"]\n",
    "    most_consistent = df.loc[df[\"Std Distance\"].idxmin(), \"Policy\"]\n",
    "    \n",
    "    print(f\"Best Overall Performance: {best_policy}\")\n",
    "    print(f\"Most Flip-Efficient: {most_efficient}\")\n",
    "    print(f\"Most Consistent: {most_consistent}\")\n",
    "    \n",
    "    # Probe analysis insights\n",
    "    print(f\"\\nObservation Insights:\")\n",
    "    print(f\"- Probe values are consistently [1.0, 1.0, 1.0], suggesting sparse obstacle placement\")\n",
    "    print(f\"- Success depends on timing gravity flips, not complex maneuvering\")\n",
    "    print(f\"- Simple heuristics perform as well as complex ones in this environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162392d2",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Environment Characteristics\n",
    "\n",
    "Our analysis reveals several important characteristics of the Gravity Guy environment:\n",
    "\n",
    "1. **Sparse Obstacle Density**: The consistent probe readings of [1.0, 1.0, 1.0] indicate that obstacles are relatively sparse, making this more about strategic timing than reactive maneuvering.\n",
    "\n",
    "2. **Well-Balanced Difficulty**: The significant gap between random (~1576px) and heuristic (~2001px) performance shows the environment rewards intelligent behavior while remaining learnable.\n",
    "\n",
    "3. **Flip Efficiency Matters**: The best policies achieve high distances with minimal flips (1-2 per episode), indicating that restraint is more important than aggressive lane-switching.\n",
    "\n",
    "### Baseline Agent Performance\n",
    "\n",
    "1. **Heuristic Dominance**: The original heuristic agent provides a strong baseline with ~27% improvement over random play.\n",
    "\n",
    "2. **Diminishing Returns**: More complex heuristics (improved, aggressive) don't outperform the simple heuristic, suggesting the original is already well-tuned for this environment.\n",
    "\n",
    "3. **Conservative Failure**: The conservative approach performs poorly due to over-flipping, demonstrating that being too cautious can be counterproductive.\n",
    "\n",
    "### Next Steps for ML Development\n",
    "\n",
    "Based on these findings, our machine learning development should focus on:\n",
    "\n",
    "#### 1. **Target Performance Goals**\n",
    "- **Beat Heuristic Baseline**: ML agents should consistently exceed 2001px average distance\n",
    "- **Reduce Variance**: Achieve more consistent performance than the 200-300px standard deviation seen in heuristics\n",
    "- **Efficiency Optimization**: Learn to achieve longer distances with fewer flips\n",
    "\n",
    "#### 2. **Recommended ML Approaches**\n",
    "\n",
    "**Deep Reinforcement Learning:**\n",
    "- **DQN**: Good starting point for discrete action learning\n",
    "- **PPO**: Should handle the sparse reward signal well\n",
    "- **Rainbow DQN**: May excel at learning long-term patterns\n",
    "\n",
    "**Imitation Learning:**\n",
    "- Use heuristic episodes as expert demonstrations for behavior cloning\n",
    "- Could provide faster initial learning than pure RL\n",
    "\n",
    "**Evolutionary Methods:**\n",
    "- Genetic algorithms might discover novel strategies\n",
    "- Less sample efficient but good for exploration\n",
    "\n",
    "#### 3. **Environment Modifications for ML**\n",
    "\n",
    "**Reward Shaping Opportunities:**\n",
    "- Add small positive rewards for maintaining good probe clearances\n",
    "- Penalty scaling based on how close to obstacles the agent gets\n",
    "- Bonus rewards for reaching distance milestones\n",
    "\n",
    "**Training Variations:**\n",
    "- Curriculum learning: start with easier (more sparse) levels\n",
    "- Multi-task learning: train on different obstacle densities\n",
    "- Self-play: train multiple agents competitively\n",
    "\n",
    "#### 4. **Evaluation Framework**\n",
    "\n",
    "**Success Metrics:**\n",
    "- Average distance > 2200px (10% improvement over heuristic)\n",
    "- Flip efficiency > 2000 pixels/flip\n",
    "- Success rate (reaching time limit) > 20%\n",
    "\n",
    "**Robustness Testing:**\n",
    "- Performance across different random seeds\n",
    "- Generalization to different level generation parameters\n",
    "- Stability across extended training runs\n",
    "\n",
    "The foundation is solid - we have a learnable environment with clear room for improvement. The next phase should focus on implementing and comparing these ML approaches against our established baseline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
