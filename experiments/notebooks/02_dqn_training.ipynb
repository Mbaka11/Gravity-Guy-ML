{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87cf55c0",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN) for Gravity Guy\n",
    "\n",
    "## What is DQN?\n",
    "**Deep Q-Network (DQN)** is a reinforcement learning method that learns a function\n",
    "$Q_\\theta(s,a)$ estimating the long-term return of taking action $a$ in state $s$.\n",
    "We act with **Œµ-greedy** (mostly pick the action with the highest Q, sometimes explore),\n",
    "and we **train** the network to match a bootstrapped target:\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "r & \\text{if episode terminated}\\\\\n",
    "r + \\gamma \\max_{a'} Q_{\\bar\\theta}(s', a') & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\mathrm{Huber}\\big( Q_\\theta(s,a) - y \\big)\n",
    "$$\n",
    "\n",
    "Two stabilizers make DQN work well in practice:\n",
    "- **Replay buffer**: learn from randomized past transitions $(s,a,r,s',\\text{done})$ to break correlations.\n",
    "- **Target network** $Q_{\\bar\\theta}$: a slowly updated copy used to compute $y$.\n",
    "\n",
    "## Why DQN fits this game\n",
    "- **Tiny, discrete action space:** 2 actions (NOOP / FLIP).\n",
    "- **Dense, shaped reward:** per-step progress minus a small flip penalty.\n",
    "- **Compact observation:** 6 floats capture what matters (vertical state + look-ahead probes).\n",
    "- **Fast, headless env:** high sample throughput for replay.\n",
    "\n",
    "## State / Action / Reward (this notebook)\n",
    "- **Observation (6 floats):**\n",
    "  1. `y_norm` ‚àà [0,1] ‚Äî vertical position (0=top, 1=bottom)  \n",
    "  2. `vy_norm` ‚àà [-1,1] ‚Äî normalized vertical speed  \n",
    "  3. `grav_dir` ‚àà {‚àí1,+1} ‚Äî current gravity (up/down)  \n",
    "  4‚Äì6. `p1, p2, p3` ‚àà [0,1] ‚Äî **look-ahead clearances** in the gravity direction (near ‚Üí far)\n",
    "- **Actions:** `0 = NOOP`, `1 = FLIP` (flip only **fires** when grounded & cooldown is over; invalid flips act as no-ops).\n",
    "- **Reward per step:** `progress ‚àí flip_penalty √ó [flip_fired]`\n",
    "- **Termination:** off-screen (death) or time limit (e.g., 10 s).\n",
    "\n",
    "## Learning loop (at a glance)\n",
    "1. **Observe** state $s$.  \n",
    "2. **Act** with Œµ-greedy: pick `argmax_a Q_\\theta(s,a)` with prob $1-Œµ$, random action with prob $Œµ$.  \n",
    "3. **Step** the env ‚Üí get $(r, s', \\text{done})$.  \n",
    "4. **Store** $(s,a,r,s',\\text{done})$ in the replay buffer.  \n",
    "5. **Sample** a mini-batch from replay, compute targets $y$ with the **target network**.  \n",
    "6. **Update** the online network $Q_\\theta$ to minimize Huber loss; periodically **update** the target network.  \n",
    "7. **Anneal** $Œµ$ over time to reduce exploration.\n",
    "\n",
    "## Game-specific caveats (and how we handle them)\n",
    "- **Action validity:** flips only take effect when grounded.  \n",
    "  *Mitigation:* treat invalid flips as no-ops and/or mask them at action selection time.\n",
    "- **Timing & partial observability:** probes look ahead in x while y changes over time.  \n",
    "  *Mitigation:* keep the observation compact but informative (probes + gravity + velocity).  \n",
    "  (Optionally, stack a few recent observations or add `grounded`/`cooldown` scalars.)\n",
    "- **Evaluation fairness:** fix a set of level seeds; report mean/median distance, % time-limit terminations, and flips per 1000 px.\n",
    "\n",
    "## What the reader should expect\n",
    "- Baselines (**Random**, **Heuristic**) for context.  \n",
    "- A DQN agent that learns to time flips better than random, often matching or surpassing the hand-crafted heuristic on held-out seeds.  \n",
    "- Clear plots: training return, evaluation distance, and failure-mode breakdown.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4871d07b",
   "metadata": {},
   "source": [
    "# 1. Environment Setup and Imports\n",
    "\n",
    "First, we'll import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1169b30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "PyTorch version: 2.8.0+cpu\n",
      "Using device: CPU\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries for ML and data handling\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "# PyTorch for neural networks (you might need: pip install torch)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Our custom environment\n",
    "import sys\n",
    "sys.path.append('../..')  # Go up two directories to access src/\n",
    "from src.env.gg_env import GGEnv\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Using device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3881d4",
   "metadata": {},
   "source": [
    "## Quick Environment Test\n",
    "\n",
    "Before building our AI, let's make sure we understand our environment perfectly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e22f0a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENVIRONMENT UNDERSTANDING ===\n",
      "Observation space: 6 dimensions\n",
      "Action space: 2 actions (0=wait, 1=flip)\n",
      "First observation: [0.5, 0.0, 1.0, 0.24814814814814815, 0.24814814814814815, 0.24814814814814815]\n",
      "Step 1: action=1, reward=2.083, done=False\n",
      "  ‚Üí obs: [0.50, 0.01, 1, 0.25, 0.25, 0.25]\n",
      "Step 2: action=0, reward=2.083, done=False\n",
      "  ‚Üí obs: [0.50, 0.03, 1, 0.25, 0.25, 0.25]\n",
      "Step 3: action=0, reward=2.083, done=False\n",
      "  ‚Üí obs: [0.50, 0.04, 1, 0.25, 0.25, 0.25]\n",
      "Step 4: action=1, reward=2.083, done=False\n",
      "  ‚Üí obs: [0.50, 0.05, 1, 0.25, 0.25, 0.25]\n",
      "Step 5: action=1, reward=2.083, done=False\n",
      "  ‚Üí obs: [0.50, 0.06, 1, 0.25, 0.25, 0.25]\n",
      "Step 6: action=1, reward=2.083, done=False\n",
      "  ‚Üí obs: [0.51, 0.07, 1, 0.24, 0.24, 0.24]\n",
      "Step 7: action=1, reward=2.083, done=False\n",
      "  ‚Üí obs: [0.51, 0.09, 1, 0.24, 0.24, 0.24]\n",
      "Step 8: action=1, reward=2.083, done=False\n",
      "  ‚Üí obs: [0.51, 0.10, 1, 0.24, 0.24, 0.24]\n",
      "Step 9: action=0, reward=2.083, done=False\n",
      "  ‚Üí obs: [0.51, 0.11, 1, 0.24, 0.24, 0.24]\n",
      "Step 10: action=0, reward=2.083, done=False\n",
      "  ‚Üí obs: [0.51, 0.12, 1, 0.24, 0.24, 0.24]\n",
      "\n",
      "‚úÖ Environment test completed!\n"
     ]
    }
   ],
   "source": [
    "# Create a test environment\n",
    "env = GGEnv(level_seed=12345, max_time_s=10.0, flip_penalty=0.01)\n",
    "obs = env.reset()\n",
    "\n",
    "print(\"=== ENVIRONMENT UNDERSTANDING ===\")\n",
    "print(f\"Observation space: {len(obs)} dimensions\")\n",
    "print(f\"Action space: {env.action_space_n} actions (0=wait, 1=flip)\")\n",
    "print(f\"First observation: {obs}\")\n",
    "\n",
    "# Take a few random actions to see what happens\n",
    "total_reward = 0\n",
    "for step in range(10):\n",
    "    action = random.choice([0, 1])  # Random action\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    print(f\"Step {step+1}: action={action}, reward={reward:.3f}, done={done}\")\n",
    "    print(f\"  ‚Üí obs: [{obs[0]:.2f}, {obs[1]:.2f}, {obs[2]:.0f}, {obs[3]:.2f}, {obs[4]:.2f}, {obs[5]:.2f}]\")\n",
    "    \n",
    "    if done:\n",
    "        print(f\"Episode ended! Total reward: {total_reward:.2f}, Distance: {info['distance_px']}px\")\n",
    "        break\n",
    "\n",
    "print(\"\\n‚úÖ Environment test completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c718570",
   "metadata": {},
   "source": [
    "## Part 2: Building the Neural Network Brain\n",
    "\n",
    "### What is our Neural Network doing?\n",
    "\n",
    "Think of the neural network as the agent's \"brain\". It takes in the 6 observations from the game and outputs 2 numbers:\n",
    "- **Q(state, wait)**: How good is it to wait/do nothing in this situation?\n",
    "- **Q(state, flip)**: How good is it to flip gravity in this situation?\n",
    "\n",
    "The agent will always choose the action with the higher Q-value.\n",
    "\n",
    "### Network Architecture Design\n",
    "\n",
    "For our Gravity Guy game, we'll use a simple but effective architecture:\n",
    "\n",
    "```\n",
    "Input Layer (6 neurons) ‚Üí Hidden Layer (128 neurons) ‚Üí Hidden Layer (64 neurons) ‚Üí Output Layer (2 neurons)\n",
    "       ‚Üì                        ‚Üì                           ‚Üì                        ‚Üì\n",
    "  [y, vy, grav,              [lots of                   [more                   [Q(wait), \n",
    "   p1, p2, p3]                neurons]                   neurons]                 Q(flip)]\n",
    "```\n",
    "\n",
    "### Why this architecture?\n",
    "- **Input**: 6 observations (exactly what our environment gives us)\n",
    "- **Hidden layers**: 128 and 64 neurons - enough to learn complex patterns but not too big to be slow\n",
    "- **Output**: 2 Q-values (one for each possible action)\n",
    "- **Activation**: ReLU (simple and effective for this type of problem)\n",
    "- **Why this size?** Small enough to be fast, large enough to capture the timing patterns in probes/velocity.\n",
    "- **Activation:** ReLU in hidden layers, **linear** in the output (Q-values can be any real number).\n",
    "- **Parameter budget (intuition):** ~6√ó128 + 128√ó64 + 64√ó2 weights (+ biases) ‚Üí ~12k parameters‚Äîtiny and trainable.\n",
    "\n",
    "\n",
    "### How it fits into the DQN loop\n",
    "1. **Forward pass:** $Q_\\theta(s)$ ‚Üí two Q-values.  \n",
    "2. **Act:** choose action via Œµ-greedy (with optional mask).  \n",
    "3. **Store:** add $(s,a,r,s',\\text{done})$ to replay.  \n",
    "4. **Target:** $y = r + \\gamma \\max_{a'} Q_{\\bar\\theta}(s',a')$ (or $y=r$ on terminal).  \n",
    "5. **Update:** minimize Huber$(Q_\\theta(s,a) - y)$.  \n",
    "6. **Stabilize:** periodically copy online weights to the **target network** (or use soft updates).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d2796",
   "metadata": {},
   "source": [
    "## Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ff2b3984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING DQN MODEL ===\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x6 and 128x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 89\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Test the forward pass with a sample observation\u001b[39;00m\n\u001b[0;32m     88\u001b[0m test_obs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.8\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 89\u001b[0m qvalues \u001b[38;5;241m=\u001b[39m \u001b[43mdqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_obs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ-values for the test observation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqvalues\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Projects\\GravityGuyML\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Projects\\GravityGuyML\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[73], line 70\u001b[0m, in \u001b[0;36mDQN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     67\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop2(x)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Add residual connection\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual\u001b[49m\u001b[43m(\u001b[49m\u001b[43midentity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m residual\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Dueling network architecture\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\GravityGuyML\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Projects\\GravityGuyML\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Projects\\GravityGuyML\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x6 and 128x128)"
     ]
    }
   ],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Enhanced Deep Q-Network with modern architecture features\n",
    "    \n",
    "    Improvements:\n",
    "    1. Deeper architecture with residual connections\n",
    "    2. Layer normalization for better training stability\n",
    "    3. Proper weight initialization\n",
    "    4. Dropout for regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_size=128):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # Input preprocessing\n",
    "        self.input_norm = nn.LayerNorm(input_size)\n",
    "        \n",
    "        # Main network layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.drop1 = nn.Dropout(0.1)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.drop2 = nn.Dropout(0.1)\n",
    "        \n",
    "        # Residual path\n",
    "        self.residual = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Output layers with value and advantage streams\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "        \n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, output_size)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights using He initialization\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize network weights using He initialization\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Ensure input is at least 2D\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        \n",
    "        # Input normalization\n",
    "        x = self.input_norm(x)\n",
    "        \n",
    "        # Main network path with residual connection\n",
    "        identity = x\n",
    "        \n",
    "        x = F.relu(self.norm1(self.fc1(x)))\n",
    "        x = self.drop1(x)\n",
    "        \n",
    "        x = F.relu(self.norm2(self.fc2(x)))\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        # Add residual connection\n",
    "        residual = self.residual(identity)\n",
    "        x = x + residual\n",
    "        \n",
    "        # Dueling network architecture\n",
    "        value = self.value_stream(x)\n",
    "        advantages = self.advantage_stream(x)\n",
    "        \n",
    "        # Combine value and advantages using dueling formula\n",
    "        qvalues = value + (advantages - advantages.mean(dim=1, keepdim=True))\n",
    "        \n",
    "        return qvalues\n",
    "    \n",
    "print(\"=== TESTING DQN MODEL ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd35dacf",
   "metadata": {},
   "source": [
    "## Understanding the Network Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "98665602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UNDERSTANDING NETWORK COMPONENTS ===\n",
      "Step-by-step forward pass:\n",
      "1. Input observations: [0.30000001192092896, -0.10000000149011612, -1.0, 0.6000000238418579, 0.699999988079071, 0.800000011920929]\n",
      "   Shape: torch.Size([1, 6])\n",
      "2. After first hidden layer (128 neurons): torch.Size([1, 128])\n",
      "   Sample values: [0.000, 0.652, 0.373, ...] (showing first 3)\n",
      "3. After second hidden layer (64 neurons): torch.Size([1, 64])\n",
      "   Sample values: [0.000, 0.000, 0.000, ...] (showing first 3)\n",
      "4. Final Q-values: [-0.21892616152763367, -0.08885392546653748]\n",
      "   Q(wait) = -0.219, Q(flip) = -0.089\n",
      "5. Decision: Choose action 1 (flip)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== UNDERSTANDING NETWORK COMPONENTS ===\")\n",
    "\n",
    "# Let's examine what each layer does\n",
    "test_obs = torch.tensor([0.3, -0.1, -1.0, 0.6, 0.7, 0.8], dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "print(\"Step-by-step forward pass:\")\n",
    "print(f\"1. Input observations: {test_obs.squeeze().tolist()}\")\n",
    "\n",
    "# Manual forward pass to see each step\n",
    "x = test_obs\n",
    "print(f\"   Shape: {x.shape}\")\n",
    "\n",
    "# Layer 1\n",
    "x = F.relu(dqn.fc1(x))  \n",
    "print(f\"2. After first hidden layer (128 neurons): {x.shape}\")\n",
    "print(f\"   Sample values: [{x[0][0]:.3f}, {x[0][1]:.3f}, {x[0][2]:.3f}, ...] (showing first 3)\")\n",
    "\n",
    "# Layer 2  \n",
    "x = F.relu(dqn.fc2(x))\n",
    "print(f\"3. After second hidden layer (64 neurons): {x.shape}\")\n",
    "print(f\"   Sample values: [{x[0][0]:.3f}, {x[0][1]:.3f}, {x[0][2]:.3f}, ...] (showing first 3)\")\n",
    "\n",
    "# Layer 3\n",
    "x = dqn.fc3(x)\n",
    "print(f\"4. Final Q-values: {x.squeeze().tolist()}\")\n",
    "print(f\"   Q(wait) = {x[0][0]:.3f}, Q(flip) = {x[0][1]:.3f}\")\n",
    "\n",
    "# Decision making\n",
    "best_action = torch.argmax(x).item()\n",
    "print(f\"5. Decision: Choose action {best_action} ({'flip' if best_action == 1 else 'wait'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7140ff",
   "metadata": {},
   "source": [
    "## Key Concepts Explained\n",
    "\n",
    "**What just happened?**\n",
    "\n",
    "1. **Input Processing**: We fed the network 6 numbers representing the game state\n",
    "2. **Hidden Layers**: The network processed this information through two layers of neurons\n",
    "3. **Q-Value Output**: We got back 2 numbers - Q(wait) and Q(flip)  \n",
    "4. **Action Selection**: We pick the action with the highest Q-value\n",
    "\n",
    "**Why ReLU activation?**\n",
    "- ReLU (Rectified Linear Unit) simply makes negative values = 0\n",
    "- It's fast, simple, and works well for most problems\n",
    "- Helps the network learn complex patterns\n",
    "\n",
    "**Why no activation on the output?**\n",
    "- Q-values can be positive or negative (good or bad situations)  \n",
    "- We want the raw values, not constrained to 0-1 range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa1cf13",
   "metadata": {},
   "source": [
    "## Part 3: Building the Agent's Memory System (Experience Replay Buffer)\n",
    "\n",
    "### What is Experience Replay?\n",
    "\n",
    "**The Problem with Naive Learning:**\n",
    "Imagine trying to learn to drive by only remembering your last 1 second of driving. You'd never learn patterns like \"red light ahead ‚Üí slow down\" because you'd forget the red light by the time you needed to brake.\n",
    "\n",
    "**The Solution - Experience Replay:**\n",
    "Instead of learning from just the current experience, we store the agent's experiences in a \"memory buffer\" and learn from random samples. This breaks the correlation between consecutive experiences and stabilizes learning.\n",
    "\n",
    "**Key Benefits:**\n",
    "1. **Breaks Correlation**: Learning from random past experiences prevents overfitting to current situation\n",
    "2. **Sample Efficiency**: Reuse valuable experiences multiple times  \n",
    "3. **Stability**: Smooths out learning by averaging over diverse situations\n",
    "\n",
    "### Experience Replay Buffer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "463b8cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING REPLAY BUFFER ===\n",
      "üß† Replay Buffer Created:\n",
      "   Capacity: 50,000 experiences\n",
      "   Memory usage: ~1.1 MB\n",
      "Buffer size after adding 100 experiences: 100\n",
      "\n",
      "=== SAMPLE BATCH (size=8) ===\n",
      "States shape: torch.Size([8, 6])\n",
      "Actions shape: torch.Size([8])\n",
      "Rewards shape: torch.Size([8])\n",
      "\n",
      "Sample states (first 3):\n",
      "  State 0: [0.5, 0.10000000149011612, 1.0, 0.800000011920929, 0.8999999761581421, 1.0]\n",
      "  State 1: [0.5, 0.10000000149011612, 1.0, 0.800000011920929, 0.8999999761581421, 1.0]\n",
      "  State 2: [0.5, 0.10000000149011612, 1.0, 0.800000011920929, 0.8999999761581421, 1.0]\n",
      "\n",
      "Sample actions: [0, 0, 0, 1, 1, 1, 1, 0]\n",
      "Sample rewards: [2.522397041320801, 2.5459907054901123, 2.131537437438965, 2.1102023124694824, 2.8620920181274414, 2.5370280742645264, 2.8412656784057617, 2.0784473419189453]\n",
      "Sample dones: [False, False, False, False, False, False, False, False]\n",
      "\n",
      "‚úÖ Replay buffer working correctly!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Experience Replay Buffer for DQN Agent\n",
    "    \n",
    "    This is the agent's \"memory\" - it stores past experiences and lets us\n",
    "    sample random batches for training. Think of it as a photo album of\n",
    "    game moments that we can learn from later.\n",
    "    \n",
    "    Each experience is a tuple: (state, action, reward, next_state, done)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=100000):\n",
    "        \"\"\"\n",
    "        Initialize the replay buffer\n",
    "        \n",
    "        Args:\n",
    "            capacity: Maximum number of experiences to store\n",
    "                     (older experiences get overwritten when full)\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "        \n",
    "        print(f\"üß† Replay Buffer Created:\")\n",
    "        print(f\"   Capacity: {capacity:,} experiences\")\n",
    "        print(f\"   Memory usage: ~{capacity * 6 * 4 / 1024 / 1024:.1f} MB\")  # Rough estimate\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store a new experience in the buffer\n",
    "        \n",
    "        Args:\n",
    "            state: Current observation (6D vector)\n",
    "            action: Action taken (0 or 1)  \n",
    "            reward: Reward received\n",
    "            next_state: Next observation after action\n",
    "            done: True if episode ended\n",
    "        \"\"\"\n",
    "        # Convert to numpy arrays for consistency\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "        next_state = np.array(next_state, dtype=np.float32)\n",
    "        \n",
    "        # Store as tuple\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size=32):\n",
    "        \"\"\"\n",
    "        Sample a random batch of experiences for training\n",
    "        \n",
    "        This is where the magic happens - we grab random past experiences\n",
    "        to train on, which breaks the correlation problem.\n",
    "        \n",
    "        Args:\n",
    "            batch_size: Number of experiences to sample\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of tensors: (states, actions, rewards, next_states, dones)\n",
    "        \"\"\"\n",
    "        if len(self.buffer) < batch_size:\n",
    "            raise ValueError(f\"Not enough experiences! Have {len(self.buffer)}, need {batch_size}\")\n",
    "        \n",
    "        # Sample random batch\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        # Unpack batch into separate arrays\n",
    "        states = np.array([e[0] for e in batch])\n",
    "        actions = np.array([e[1] for e in batch]) \n",
    "        rewards = np.array([e[2] for e in batch])\n",
    "        next_states = np.array([e[3] for e in batch])\n",
    "        dones = np.array([e[4] for e in batch])\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long)  # Long for indexing\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.bool)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return current number of experiences stored\"\"\"\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def is_ready(self, min_size=1000):\n",
    "        \"\"\"Check if buffer has enough experiences to start training\"\"\"\n",
    "        return len(self.buffer) >= min_size\n",
    "\n",
    "# Test the replay buffer\n",
    "print(\"=== TESTING REPLAY BUFFER ===\")\n",
    "\n",
    "# Create buffer\n",
    "replay_buffer = ReplayBuffer(capacity=50000)\n",
    "\n",
    "# Add some fake experiences\n",
    "for i in range(100):\n",
    "    state = [0.5, 0.1, 1.0, 0.8, 0.9, 1.0]  # Fake observation\n",
    "    action = random.choice([0, 1])           # Random action\n",
    "    reward = 2.0 + random.random()           # Small reward variation\n",
    "    next_state = [0.51, 0.12, 1.0, 0.75, 0.85, 0.95]  # Slightly different\n",
    "    done = (i % 50 == 49)                    # Episode ends every 50 steps\n",
    "    \n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "print(f\"Buffer size after adding 100 experiences: {len(replay_buffer)}\")\n",
    "\n",
    "# Test sampling\n",
    "if replay_buffer.is_ready(min_size=32):\n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size=8)\n",
    "    \n",
    "    print(f\"\\n=== SAMPLE BATCH (size=8) ===\")\n",
    "    print(f\"States shape: {states.shape}\")\n",
    "    print(f\"Actions shape: {actions.shape}\")\n",
    "    print(f\"Rewards shape: {rewards.shape}\")\n",
    "    \n",
    "    print(f\"\\nSample states (first 3):\")\n",
    "    for i in range(3):\n",
    "        print(f\"  State {i}: {states[i].tolist()}\")\n",
    "    \n",
    "    print(f\"\\nSample actions: {actions.tolist()}\")\n",
    "    print(f\"Sample rewards: {rewards.tolist()}\")\n",
    "    print(f\"Sample dones: {dones.tolist()}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Replay buffer working correctly!\")\n",
    "else:\n",
    "    print(\"‚ùå Not enough experiences to sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb6afa",
   "metadata": {},
   "source": [
    "### Understanding the Memory System\n",
    "\n",
    "**Key Concepts Explained:**\n",
    "\n",
    "1. **Capacity Management**: \n",
    "   - Buffer has fixed size (100,000 experiences)\n",
    "   - When full, oldest experiences are overwritten\n",
    "   - This prevents memory from growing infinitely\n",
    "\n",
    "2. **Experience Format**:\n",
    "   ```python\n",
    "   (state, action, reward, next_state, done)\n",
    "   # Example: ([0.5, 0.1, 1.0, 0.8, 0.9, 1.0], 1, 2.1, [0.51, 0.12, 1.0, 0.75, 0.85, 0.95], False)\n",
    "   ```\n",
    "\n",
    "3. **Random Sampling**:\n",
    "   - We don't learn from experiences in order\n",
    "   - Random sampling breaks temporal correlations\n",
    "   - Each training batch has diverse situations\n",
    "\n",
    "4. **Batch Processing**:\n",
    "   - Sample multiple experiences at once (e.g., 32)\n",
    "   - More efficient than training on single experiences\n",
    "   - Provides better gradient estimates\n",
    "\n",
    "### Why This Matters for Gravity Guy\n",
    "\n",
    "**Without Replay Buffer:**\n",
    "- Agent only learns from current situation\n",
    "- Forgets valuable lessons from past mistakes\n",
    "- Training is unstable and inefficient\n",
    "\n",
    "**With Replay Buffer:**\n",
    "- Learns from diverse situations (different platform layouts, gravity states)\n",
    "- Remembers rare but important events (close calls, successful flips)\n",
    "- Training is stable and data-efficient\n",
    "\n",
    "**Memory Efficiency:**\n",
    "- 100,000 experiences ‚âà 25MB of memory (very reasonable)\n",
    "- Enough to store ~30 minutes of gameplay at 60 FPS\n",
    "- Captures diverse situations for robust learning\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Now that we have our replay buffer, we need:\n",
    "1. **DQN Agent Class** - Combines neural network + replay buffer\n",
    "2. **Training Loop** - The learning process\n",
    "3. **Epsilon-Greedy Policy** - Balancing exploration vs exploitation\n",
    "\n",
    "The replay buffer is the foundation that makes everything else possible!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce16f9e",
   "metadata": {},
   "source": [
    "## Part 4: The Complete DQN Agent\n",
    "\n",
    "### What We're Building\n",
    "\n",
    "Now we combine everything into a complete learning agent:\n",
    "- **Brain**: Neural network (from Part 2) \n",
    "- **Memory**: Replay buffer (from Part 3)\n",
    "- **Decision Making**: Epsilon-greedy exploration\n",
    "- **Learning**: DQN training algorithm\n",
    "\n",
    "Think of this as assembling all the pieces into a complete AI player.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**üß† Epsilon-Greedy Strategy:**\n",
    "- **Exploration**: Sometimes take random actions to discover new strategies\n",
    "- **Exploitation**: Usually take the action our network thinks is best\n",
    "- **Balance**: Start with high exploration (90%), gradually reduce to low (5%)\n",
    "\n",
    "**üéØ DQN Learning Process:**\n",
    "1. Observe current state\n",
    "2. Choose action (epsilon-greedy)\n",
    "3. Take action, get reward and new state\n",
    "4. Store experience in replay buffer\n",
    "5. Sample random batch from memory\n",
    "6. Train network on batch\n",
    "7. Repeat!\n",
    "\n",
    "### Complete DQN Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14163c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING COMPLETE DQN AGENT ===\n",
      "üß† DQN Network Created:\n",
      "   Input: 6 ‚Üí Hidden: 128 ‚Üí Hidden: 64 ‚Üí Output: 2\n",
      "   Total parameters: 9,282\n",
      "üß† DQN Network Created:\n",
      "   Input: 6 ‚Üí Hidden: 128 ‚Üí Hidden: 64 ‚Üí Output: 2\n",
      "   Total parameters: 9,282\n",
      "üß† Replay Buffer Created:\n",
      "   Capacity: 10,000 experiences\n",
      "   Memory usage: ~0.2 MB\n",
      "ü§ñ DQN Agent Initialized:\n",
      "   State/Action space: 6 ‚Üí 2\n",
      "   Learning rate: 0.001\n",
      "   Exploration: 0.90 ‚Üí 0.05\n",
      "   Buffer size: 10,000\n",
      "   Batch size: 32\n",
      "\n",
      "=== TESTING ACTION SELECTION ===\n",
      "State 1: explore=1, exploit=0\n",
      "State 2: explore=0, exploit=0\n",
      "State 3: explore=0, exploit=0\n",
      "\n",
      "=== TESTING LEARNING PROCESS ===\n",
      "Agent trained on 100 experiences\n",
      "Current exploration rate: 0.545\n",
      "Memory size: 100\n",
      "Training steps completed: 100\n",
      "\n",
      "‚úÖ DQN Agent is ready for training!\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    Complete Deep Q-Network Agent for Gravity Guy\n",
    "    \n",
    "    This agent combines:\n",
    "    - Neural network for decision making\n",
    "    - Replay buffer for experience storage  \n",
    "    - Epsilon-greedy exploration strategy\n",
    "    - DQN training algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size=6,\n",
    "        action_size=2, \n",
    "        learning_rate=0.0003,\n",
    "        gamma=0.99,\n",
    "        epsilon_start=0.95,\n",
    "        epsilon_end=0.05,\n",
    "        epsilon_decay=0.995,\n",
    "        buffer_size=50000,\n",
    "        batch_size=32,\n",
    "        target_update_freq=1000\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the DQN agent\n",
    "        \n",
    "        Args:\n",
    "            state_size: Size of observation space (6 for Gravity Guy)\n",
    "            action_size: Number of actions (2: wait/flip)\n",
    "            learning_rate: How fast the network learns\n",
    "            gamma: Discount factor (how much future rewards matter)\n",
    "            epsilon_start: Initial exploration rate (95% random actions)\n",
    "            epsilon_end: Final exploration rate (5% random actions)  \n",
    "            epsilon_decay: How quickly we reduce exploration\n",
    "            buffer_size: Replay buffer capacity\n",
    "            batch_size: Number of experiences per training batch\n",
    "            target_update_freq: How often to update target network\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        \n",
    "        # Exploration parameters\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        # Neural networks (main and target)\n",
    "        self.q_network = DQN(state_size, 128, 64, action_size)\n",
    "        self.target_network = DQN(state_size, 128, 64, action_size)\n",
    "        \n",
    "        # Initialize target network with same weights as main network\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # Optimizer for training\n",
    "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Experience replay buffer\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        # Training step counter\n",
    "        self.steps_done = 0\n",
    "        \n",
    "        print(f\"ü§ñ DQN Agent Initialized:\")\n",
    "        print(f\"   State/Action space: {state_size} ‚Üí {action_size}\")\n",
    "        print(f\"   Learning rate: {learning_rate}\")\n",
    "        print(f\"   Exploration: {epsilon_start:.2f} ‚Üí {epsilon_end:.2f}\")\n",
    "        print(f\"   Buffer size: {buffer_size:,}\")\n",
    "        print(f\"   Batch size: {batch_size}\")\n",
    "    \n",
    "    def act(self, state, training=True):\n",
    "        \"\"\"\n",
    "        Choose action using epsilon-greedy strategy\n",
    "        \n",
    "        Args:\n",
    "            state: Current observation (6D vector)\n",
    "            training: If False, always exploit (no exploration)\n",
    "            \n",
    "        Returns:\n",
    "            action: 0 (wait) or 1 (flip)\n",
    "        \"\"\"\n",
    "        # Convert state to tensor\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if training and random.random() < self.epsilon:\n",
    "            # Explore: random action\n",
    "            action = random.choice([0, 1])\n",
    "        else:\n",
    "            # Exploit: best action according to Q-network\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state)\n",
    "                action = torch.argmax(q_values[0]).item()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store experience and learn from replay buffer\n",
    "        \n",
    "        Args:\n",
    "            state: Previous observation\n",
    "            action: Action taken\n",
    "            reward: Reward received  \n",
    "            next_state: New observation\n",
    "            done: True if episode ended\n",
    "        \"\"\"\n",
    "        # Store experience in replay buffer\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Increment step counter\n",
    "        self.steps_done += 1\n",
    "        \n",
    "        # Learn from experiences (if we have enough)\n",
    "        if self.memory.is_ready(self.batch_size):\n",
    "            experiences = self.memory.sample(self.batch_size)\n",
    "            self._learn(experiences)\n",
    "        \n",
    "        # Update target network periodically\n",
    "        if self.steps_done % self.target_update_freq == 0:\n",
    "            self._update_target_network()\n",
    "        \n",
    "        # Decay exploration rate\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def _learn(self, experiences):\n",
    "        \"\"\"\n",
    "        Learn from a batch of experiences using DQN algorithm\n",
    "        \n",
    "        This is where the actual learning happens!\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        # Get current Q-values for chosen actions\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Get next Q-values from target network\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            # Set next Q-values to 0 for terminal states\n",
    "            next_q_values[dones] = 0.0\n",
    "        \n",
    "        # Compute target Q-values\n",
    "        target_q_values = rewards + (self.gamma * next_q_values)\n",
    "        \n",
    "        # Compute loss (Huber loss for stability)\n",
    "        loss = F.smooth_l1_loss(current_q_values.squeeze(), target_q_values)\n",
    "        \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def _update_target_network(self):\n",
    "        \"\"\"Copy weights from main network to target network\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        print(f\"üéØ Target network updated at step {self.steps_done}\")\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save the trained model\"\"\"\n",
    "        torch.save({\n",
    "            'q_network_state_dict': self.q_network.state_dict(),\n",
    "            'target_network_state_dict': self.target_network.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon,\n",
    "            'steps_done': self.steps_done\n",
    "        }, filepath)\n",
    "        print(f\"üíæ Model saved to {filepath}\")\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load a trained model\"\"\"\n",
    "        checkpoint = torch.load(filepath)\n",
    "        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "        self.steps_done = checkpoint['steps_done']\n",
    "        print(f\"üìÇ Model loaded from {filepath}\")\n",
    "\n",
    "# Test the complete agent\n",
    "print(\"=== TESTING COMPLETE DQN AGENT ===\")\n",
    "\n",
    "# Create agent\n",
    "agent = DQNAgent(\n",
    "    state_size=6,\n",
    "    action_size=2,\n",
    "    learning_rate=0.001,\n",
    "    epsilon_start=0.9,\n",
    "    epsilon_end=0.05,\n",
    "    buffer_size=10000,  # Smaller for testing\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(f\"\\n=== TESTING ACTION SELECTION ===\")\n",
    "\n",
    "# Test action selection\n",
    "test_states = [\n",
    "    [0.3, -0.1, 1.0, 0.8, 0.9, 1.0],  # Safe situation\n",
    "    [0.7, 0.2, -1.0, 0.2, 0.3, 0.4],  # Dangerous situation  \n",
    "    [0.5, 0.0, 1.0, 1.0, 1.0, 1.0],   # Very safe\n",
    "]\n",
    "\n",
    "for i, state in enumerate(test_states):\n",
    "    action_explore = agent.act(state, training=True)   # With exploration\n",
    "    action_exploit = agent.act(state, training=False)  # Pure exploitation\n",
    "    \n",
    "    print(f\"State {i+1}: explore={action_explore}, exploit={action_exploit}\")\n",
    "\n",
    "print(f\"\\n=== TESTING LEARNING PROCESS ===\")\n",
    "\n",
    "# Add some experiences and test learning\n",
    "for i in range(100):\n",
    "    state = [0.5 + 0.1*random.random(), 0.1*random.random(), \n",
    "             random.choice([-1, 1]), 0.8*random.random(), \n",
    "             0.8*random.random(), 0.8*random.random()]\n",
    "    action = random.choice([0, 1])\n",
    "    reward = 2.0 + random.random()\n",
    "    next_state = [s + 0.01*random.random() for s in state]\n",
    "    done = (i % 25 == 24)\n",
    "    \n",
    "    agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "print(f\"Agent trained on 100 experiences\")\n",
    "print(f\"Current exploration rate: {agent.epsilon:.3f}\")\n",
    "print(f\"Memory size: {len(agent.memory)}\")\n",
    "print(f\"Training steps completed: {agent.steps_done}\")\n",
    "\n",
    "print(f\"\\n‚úÖ DQN Agent is ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618a7d83",
   "metadata": {},
   "source": [
    "**üß† The Learning Process:**\n",
    "\n",
    "1. **Observe**: Get current game state (6D vector)\n",
    "2. **Decide**: Use epsilon-greedy to pick action\n",
    "3. **Act**: Take action in environment  \n",
    "4. **Remember**: Store experience in replay buffer\n",
    "5. **Learn**: Train on random batch from memory\n",
    "6. **Improve**: Update exploration rate and target network\n",
    "\n",
    "**üéØ Key Components:**\n",
    "\n",
    "- **Main Network**: Makes decisions and gets trained\n",
    "- **Target Network**: Provides stable learning targets (updated every 1000 steps)\n",
    "- **Epsilon Decay**: Exploration decreases over time (95% ‚Üí 5%)\n",
    "- **Experience Replay**: Learns from diverse past experiences\n",
    "\n",
    "**üöÄ Why This Works for Gravity Guy:**\n",
    "\n",
    "- **Exploration**: Discovers new flipping strategies\n",
    "- **Memory**: Remembers successful and failed situations  \n",
    "- **Stability**: Target network prevents learning instability\n",
    "- **Efficiency**: Batch training is much faster than single updates\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Your agent is now complete! Next we need:\n",
    "1. **Training Loop** - Connect agent to your environment\n",
    "2. **Performance Monitoring** - Track learning progress\n",
    "3. **Evaluation** - Test against baseline agents\n",
    "\n",
    "Ready to see this agent learn to play Gravity Guy? üéÆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc92cf28",
   "metadata": {},
   "source": [
    "## Part 5: Training the Improved DQN Agent\n",
    "\n",
    "### Key Improvements\n",
    "\n",
    "1. **Reward Structure**:\n",
    "   - Scale down progress rewards\n",
    "   - Add efficiency-based flip penalties\n",
    "   - Encourage momentum preservation\n",
    "\n",
    "2. **State Space**:\n",
    "   - Add grounded state\n",
    "   - Add cooldown information\n",
    "   - Normalize input features\n",
    "\n",
    "3. **Network Architecture**:\n",
    "   - Add batch normalization\n",
    "   - Better weight initialization\n",
    "   - Entropy regularization\n",
    "\n",
    "4. **Training Process**:\n",
    "   - Slower learning rate\n",
    "   - More exploration time\n",
    "   - Larger replay buffer\n",
    "\n",
    "Let's implement these improvements and see how they affect learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "924dbf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING IMPROVED DQN AGENT ===\n",
      "üß† Improved DQN Created:\n",
      "   Input: 8 ‚Üí Hidden: 128 ‚Üí Hidden: 64 ‚Üí Output: 2\n",
      "   Total parameters: 9,922\n",
      "üß† Improved DQN Created:\n",
      "   Input: 8 ‚Üí Hidden: 128 ‚Üí Hidden: 64 ‚Üí Output: 2\n",
      "   Total parameters: 9,922\n",
      "üß† Improved Replay Buffer Created:\n",
      "   Capacity: 100,000 experiences\n",
      "   Memory usage: ~3.1 MB\n",
      "ü§ñ Improved DQN Agent Initialized:\n",
      "   State/Action space: 8 ‚Üí 2\n",
      "   Learning rate: 0.0001\n",
      "   Exploration: 0.99 ‚Üí 0.01\n",
      "   Buffer size: 100,000\n",
      "   Batch size: 32\n",
      "\n",
      "‚úÖ Improved agent created and ready for training!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import math\n",
    "\n",
    "class ImprovedDQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved Deep Q-Network with batch normalization and better initialization\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=8, hidden1_size=128, hidden2_size=64, output_size=2):\n",
    "        super(ImprovedDQN, self).__init__()\n",
    "        \n",
    "        # Network layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden1_size)\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden2_size)\n",
    "        self.fc3 = nn.Linear(hidden2_size, output_size)\n",
    "        \n",
    "        # Initialize with small weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "        print(f\"üß† Improved DQN Created:\")\n",
    "        print(f\"   Input: {input_size} ‚Üí Hidden: {hidden1_size} ‚Üí Hidden: {hidden2_size} ‚Üí Output: {output_size}\")\n",
    "        print(f\"   Total parameters: {sum(p.numel() for p in self.parameters()):,}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input normalization\n",
    "        if self.training:\n",
    "            x = (x - x.mean(dim=0)) / (x.std(dim=0) + 1e-8)\n",
    "        \n",
    "        # First hidden layer\n",
    "        x = self.fc1(x)\n",
    "        if x.shape[0] > 1:  # Only use batch norm for batches\n",
    "            x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Second hidden layer\n",
    "        x = self.fc2(x)\n",
    "        if x.shape[0] > 1:\n",
    "            x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Output layer (no activation - raw Q-values)\n",
    "        return self.fc3(x)\n",
    "\n",
    "class ImprovedReplayBuffer:\n",
    "    \"\"\"\n",
    "    Enhanced Replay Buffer with prioritized sampling and better memory management\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.priorities = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "        self.epsilon = 0.01  # Small constant to ensure non-zero priorities\n",
    "        self.alpha = 0.6     # Priority exponent\n",
    "        self.beta = 0.4      # Importance sampling weight\n",
    "        self.beta_increment = 0.001\n",
    "        \n",
    "        print(f\"üß† Improved Replay Buffer Created:\")\n",
    "        print(f\"   Capacity: {capacity:,} experiences\")\n",
    "        print(f\"   Memory usage: ~{capacity * 8 * 4 / 1024 / 1024:.1f} MB\")\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience with max priority for new experiences\"\"\"\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "        next_state = np.array(next_state, dtype=np.float32)\n",
    "        \n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        max_priority = max(self.priorities, default=1.0)\n",
    "        \n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(max_priority)\n",
    "    \n",
    "    def sample(self, batch_size=32):\n",
    "        \"\"\"Sample batch with prioritized experience replay\"\"\"\n",
    "        total_priority = sum(self.priorities)\n",
    "        probs = [p / total_priority for p in self.priorities]\n",
    "        \n",
    "        # Sample indices based on priorities\n",
    "        indices = np.random.choice(\n",
    "            len(self.buffer), \n",
    "            batch_size, \n",
    "            p=probs,\n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        # Get experiences and calculate importance weights\n",
    "        experiences = [self.buffer[i] for i in indices]\n",
    "        total = len(self.buffer)\n",
    "        weights = [(total * probs[i]) ** -self.beta for i in indices]\n",
    "        max_weight = max(weights)\n",
    "        weights = [w / max_weight for w in weights]  # Normalize weights\n",
    "        \n",
    "        # Increment beta\n",
    "        self.beta = min(1.0, self.beta + self.beta_increment)\n",
    "        \n",
    "        # Unpack experiences\n",
    "        states = np.array([e[0] for e in experiences])\n",
    "        actions = np.array([e[1] for e in experiences])\n",
    "        rewards = np.array([e[2] for e in experiences])\n",
    "        next_states = np.array([e[3] for e in experiences])\n",
    "        dones = np.array([e[4] for e in experiences])\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.bool)\n",
    "        weights = torch.tensor(weights, dtype=torch.float32)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, weights, indices\n",
    "    \n",
    "    def update_priorities(self, indices, td_errors):\n",
    "        \"\"\"Update priorities based on TD errors\"\"\"\n",
    "        for idx, error in zip(indices, td_errors):\n",
    "            self.priorities[idx] = (abs(error) + self.epsilon) ** self.alpha\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def is_ready(self, min_size=1000):\n",
    "        return len(self.buffer) >= min_size\n",
    "\n",
    "class ImprovedDQNAgent:\n",
    "    \"\"\"\n",
    "    Enhanced DQN Agent with:\n",
    "    - Prioritized experience replay\n",
    "    - Double DQN\n",
    "    - Dueling architecture\n",
    "    - Entropy regularization\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size=8,\n",
    "        action_size=2,\n",
    "        learning_rate=0.0001,\n",
    "        gamma=0.99,\n",
    "        epsilon_start=0.99,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=0.9999,\n",
    "        buffer_size=100000,\n",
    "        batch_size=32,\n",
    "        target_update_freq=100\n",
    "    ):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        \n",
    "        # Exploration parameters\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        # Networks\n",
    "        self.q_network = ImprovedDQN(state_size, 128, 64, action_size)\n",
    "        self.target_network = ImprovedDQN(state_size, 128, 64, action_size)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # Optimizer with gradient clipping\n",
    "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Enhanced replay buffer\n",
    "        self.memory = ImprovedReplayBuffer(buffer_size)\n",
    "        \n",
    "        # Training tracking\n",
    "        self.steps_done = 0\n",
    "        self.losses = []\n",
    "        \n",
    "        print(f\"ü§ñ Improved DQN Agent Initialized:\")\n",
    "        print(f\"   State/Action space: {state_size} ‚Üí {action_size}\")\n",
    "        print(f\"   Learning rate: {learning_rate}\")\n",
    "        print(f\"   Exploration: {epsilon_start:.2f} ‚Üí {epsilon_end:.2f}\")\n",
    "        print(f\"   Buffer size: {buffer_size:,}\")\n",
    "        print(f\"   Batch size: {batch_size}\")\n",
    "    \n",
    "    def act(self, state, training=True):\n",
    "        \"\"\"Choose action using epsilon-greedy with validity masking\"\"\"\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        \n",
    "        # Get valid actions mask (from state[3] = grounded)\n",
    "        can_flip = bool(state[0, 3].item() and state[0, 4].item())  # grounded and cooldown ready\n",
    "        \n",
    "        # Epsilon-greedy with validity masking\n",
    "        if training and random.random() < self.epsilon:\n",
    "            if can_flip:\n",
    "                return random.choice([0, 1])\n",
    "            else:\n",
    "                return 0  # Can only wait\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state)\n",
    "                if not can_flip:\n",
    "                    q_values[0, 1] = float('-inf')  # Mask out flip action\n",
    "                return torch.argmax(q_values[0]).item()\n",
    "    \n",
    "    def calculate_loss(self, states, actions, rewards, next_states, dones, weights):\n",
    "        \"\"\"Calculate loss with entropy regularization\"\"\"\n",
    "        # Current Q-values\n",
    "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Double DQN: use online network to select action, target network to evaluate\n",
    "            online_next_q = self.q_network(next_states)\n",
    "            target_next_q = self.target_network(next_states)\n",
    "            \n",
    "            # Get best actions from online network\n",
    "            best_actions = torch.argmax(online_next_q, dim=1, keepdim=True)\n",
    "            \n",
    "            # Use those actions to get Q-values from target network\n",
    "            next_q = target_next_q.gather(1, best_actions).squeeze()\n",
    "            \n",
    "            # Add entropy regularization\n",
    "            probs = F.softmax(target_next_q, dim=1)\n",
    "            entropy = -0.01 * (probs * torch.log(probs + 1e-10)).sum(1)\n",
    "            next_q = next_q + entropy\n",
    "            \n",
    "            # Compute targets\n",
    "            target_q = rewards + (1 - dones.float()) * self.gamma * next_q\n",
    "        \n",
    "        # Compute weighted Huber loss\n",
    "        td_errors = F.smooth_l1_loss(current_q.squeeze(), target_q, reduction='none')\n",
    "        loss = (weights * td_errors).mean()\n",
    "        \n",
    "        return loss, td_errors.detach()\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Process one step of experience\"\"\"\n",
    "        # Store experience\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Increment step counter\n",
    "        self.steps_done += 1\n",
    "        \n",
    "        # Learn if we have enough experiences\n",
    "        if self.memory.is_ready(self.batch_size):\n",
    "            # Sample batch with priorities\n",
    "            states, actions, rewards, next_states, dones, weights, indices = \\\n",
    "                self.memory.sample(self.batch_size)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss, td_errors = self.calculate_loss(\n",
    "                states, actions, rewards, next_states, dones, weights\n",
    "            )\n",
    "            \n",
    "            # Update priorities\n",
    "            self.memory.update_priorities(indices, td_errors.numpy())\n",
    "            \n",
    "            # Optimize\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Store loss\n",
    "            self.losses.append(loss.item())\n",
    "        \n",
    "        # Update target network\n",
    "        if self.steps_done % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "            print(f\"üéØ Target network updated at step {self.steps_done}\")\n",
    "        \n",
    "        # Decay exploration rate\n",
    "        self.epsilon = max(\n",
    "            self.epsilon_end,\n",
    "            self.epsilon * self.epsilon_decay\n",
    "        )\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save the trained model\"\"\"\n",
    "        torch.save({\n",
    "            'q_network_state_dict': self.q_network.state_dict(),\n",
    "            'target_network_state_dict': self.target_network.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon,\n",
    "            'steps_done': self.steps_done,\n",
    "            'losses': self.losses\n",
    "        }, filepath)\n",
    "        print(f\"üíæ Model saved to {filepath}\")\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load a trained model\"\"\"\n",
    "        checkpoint = torch.load(filepath)\n",
    "        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "        self.steps_done = checkpoint['steps_done']\n",
    "        self.losses = checkpoint['losses']\n",
    "        print(f\"üìÇ Model loaded from {filepath}\")\n",
    "\n",
    "# Create the improved agent\n",
    "print(\"=== CREATING IMPROVED DQN AGENT ===\")\n",
    "\n",
    "improved_agent = ImprovedDQNAgent(\n",
    "    state_size=8,              # Extended state space\n",
    "    action_size=2,\n",
    "    learning_rate=0.0001,      # Slower learning\n",
    "    gamma=0.99,                # High discount\n",
    "    epsilon_start=0.99,        # More exploration\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=0.9999,      # Much slower decay\n",
    "    buffer_size=100000,        # 2x larger buffer\n",
    "    batch_size=32,\n",
    "    target_update_freq=100     # More frequent updates\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Improved agent created and ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126ecda3",
   "metadata": {},
   "source": [
    "### Training Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "05b70ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STARTING IMPROVED DQN TRAINING ===\n",
      "This will take 20-25 minutes to train for 500 episodes.\n",
      "You'll see much more detailed progress updates and metrics.\n",
      "------------------------------------------------------------\n",
      "üöÄ Starting Improved DQN Training!\n",
      "Episodes: 500\n",
      "Target: Beat heuristic baseline of 2001px\n",
      "Environment: 30s episodes, random levels, 0.1 flip penalty\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x6 and 8x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 347\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# Training run\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m training_monitor \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_improved_dqn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimproved_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Full training run\u001b[39;49;00m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps_per_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 30 seconds at 120 FPS\u001b[39;49;00m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Frequent updates\u001b[39;49;00m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Regular plotting\u001b[39;49;00m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Regular checkpoints\u001b[39;49;00m\n\u001b[0;32m    354\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[69], line 240\u001b[0m, in \u001b[0;36mtrain_improved_dqn\u001b[1;34m(agent, n_episodes, max_steps_per_episode, print_every, plot_every, save_every, eval_every)\u001b[0m\n\u001b[0;32m    237\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m progress\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# Agent learns from experience\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# Store loss if available\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mlosses:\n",
      "Cell \u001b[1;32mIn[67], line 255\u001b[0m, in \u001b[0;36mImprovedDQNAgent.step\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m    251\u001b[0m states, actions, rewards, next_states, dones, weights, indices \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m--> 255\u001b[0m loss, td_errors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Update priorities\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mupdate_priorities(indices, td_errors\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "Cell \u001b[1;32mIn[67], line 213\u001b[0m, in \u001b[0;36mImprovedDQNAgent.calculate_loss\u001b[1;34m(self, states, actions, rewards, next_states, dones, weights)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calculate loss with entropy regularization\"\"\"\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# Current Q-values\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m current_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# Double DQN: use online network to select action, target network to evaluate\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     online_next_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_network(next_states)\n",
      "File \u001b[1;32md:\\Projects\\GravityGuyML\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Projects\\GravityGuyML\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[67], line 39\u001b[0m, in \u001b[0;36mImprovedDQN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     36\u001b[0m     x \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m-\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;241m/\u001b[39m (x\u001b[38;5;241m.\u001b[39mstd(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# First hidden layer\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# Only use batch norm for batches\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n",
      "File \u001b[1;32md:\\Projects\\GravityGuyML\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Projects\\GravityGuyML\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Projects\\GravityGuyML\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x6 and 8x128)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImprovedTrainingMonitor:\n",
    "    \"\"\"\n",
    "    Enhanced training progress tracking with more detailed metrics\n",
    "    \"\"\"\n",
    "    def __init__(self, window_size=100):\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Core metrics\n",
    "        self.episode_rewards = []\n",
    "        self.episode_distances = []\n",
    "        self.episode_flips = []\n",
    "        self.episode_lengths = []\n",
    "        self.epsilons = []\n",
    "        self.losses = []\n",
    "        \n",
    "        # Detailed metrics\n",
    "        self.flip_efficiencies = []  # Distance gained per flip\n",
    "        self.survival_times = []     # Episode duration\n",
    "        self.q_value_gaps = []       # Difference between Q(flip) and Q(wait)\n",
    "        \n",
    "        # Rolling windows\n",
    "        self.reward_window = deque(maxlen=window_size)\n",
    "        self.distance_window = deque(maxlen=window_size)\n",
    "        self.efficiency_window = deque(maxlen=window_size)\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.best_distance = 0\n",
    "        self.best_efficiency = 0\n",
    "        self.best_episode = 0\n",
    "    \n",
    "    def update(self, episode, reward, distance, flips, length, epsilon, loss=None, q_values=None):\n",
    "        \"\"\"Record metrics from completed episode\"\"\"\n",
    "        # Core metrics\n",
    "        self.episode_rewards.append(reward)\n",
    "        self.episode_distances.append(distance)\n",
    "        self.episode_flips.append(flips)\n",
    "        self.episode_lengths.append(length)\n",
    "        self.epsilons.append(epsilon)\n",
    "        if loss is not None:\n",
    "            self.losses.append(loss)\n",
    "        \n",
    "        # Calculate efficiency\n",
    "        efficiency = distance / max(flips, 1)\n",
    "        self.flip_efficiencies.append(efficiency)\n",
    "        \n",
    "        # Calculate Q-value gap if available\n",
    "        if q_values is not None:\n",
    "            gap = abs(q_values[1] - q_values[0])  # Gap between flip and wait\n",
    "            self.q_value_gaps.append(gap)\n",
    "        \n",
    "        # Update rolling windows\n",
    "        self.reward_window.append(reward)\n",
    "        self.distance_window.append(distance)\n",
    "        self.efficiency_window.append(efficiency)\n",
    "        \n",
    "        # Track best performance\n",
    "        if distance > self.best_distance:\n",
    "            self.best_distance = distance\n",
    "            self.best_episode = episode\n",
    "        if efficiency > self.best_efficiency:\n",
    "            self.best_efficiency = efficiency\n",
    "    \n",
    "    def get_averages(self):\n",
    "        \"\"\"Get current rolling averages\"\"\"\n",
    "        avg_reward = np.mean(self.reward_window) if self.reward_window else 0\n",
    "        avg_distance = np.mean(self.distance_window) if self.distance_window else 0\n",
    "        avg_efficiency = np.mean(self.efficiency_window) if self.efficiency_window else 0\n",
    "        return avg_reward, avg_distance, avg_efficiency\n",
    "    \n",
    "    def print_progress(self, episode, verbose=True):\n",
    "        \"\"\"Print detailed training progress\"\"\"\n",
    "        if verbose or episode % 50 == 0:\n",
    "            avg_reward, avg_distance, avg_efficiency = self.get_averages()\n",
    "            current_epsilon = self.epsilons[-1] if self.epsilons else 0\n",
    "            recent_flips = np.mean(self.episode_flips[-self.window_size:])\n",
    "            \n",
    "            print(f\"Episode {episode:4d} | \"\n",
    "                  f\"Distance: {self.episode_distances[-1]:4.0f}px | \"\n",
    "                  f\"Avg Dist: {avg_distance:4.0f}px | \"\n",
    "                  f\"Flips: {self.episode_flips[-1]:2d} | \"\n",
    "                  f\"Efficiency: {avg_efficiency:4.0f} px/flip | \"\n",
    "                  f\"Œµ: {current_epsilon:.3f}\")\n",
    "    \n",
    "    def plot_training(self, show_baselines=True):\n",
    "        \"\"\"Generate comprehensive training visualization\"\"\"\n",
    "        fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "        fig.suptitle('Improved DQN Training Progress', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        episodes = range(len(self.episode_distances))\n",
    "        \n",
    "        # 1. Distance Performance\n",
    "        ax1 = axes[0, 0]\n",
    "        ax1.plot(episodes, self.episode_distances, alpha=0.3, color='blue')\n",
    "        if len(self.episode_distances) >= 50:\n",
    "            rolling_dist = np.convolve(self.episode_distances, \n",
    "                                     np.ones(50)/50, mode='valid')\n",
    "            ax1.plot(range(49, len(episodes)), rolling_dist, \n",
    "                    color='blue', linewidth=2, label='50-episode average')\n",
    "        \n",
    "        if show_baselines:\n",
    "            ax1.axhline(y=1576, color='red', linestyle='--', \n",
    "                       alpha=0.7, label='Random baseline')\n",
    "            ax1.axhline(y=2001, color='green', linestyle='--', \n",
    "                       alpha=0.7, label='Heuristic baseline')\n",
    "        \n",
    "        ax1.set_title('Distance Performance')\n",
    "        ax1.set_xlabel('Episode')\n",
    "        ax1.set_ylabel('Distance (px)')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Flip Efficiency\n",
    "        ax2 = axes[0, 1]\n",
    "        ax2.plot(episodes, self.flip_efficiencies, alpha=0.3, color='green')\n",
    "        if len(self.flip_efficiencies) >= 50:\n",
    "            rolling_eff = np.convolve(self.flip_efficiencies, \n",
    "                                    np.ones(50)/50, mode='valid')\n",
    "            ax2.plot(range(49, len(episodes)), rolling_eff, \n",
    "                    color='green', linewidth=2, label='50-episode average')\n",
    "        \n",
    "        ax2.set_title('Flip Efficiency (Distance per Flip)')\n",
    "        ax2.set_xlabel('Episode')\n",
    "        ax2.set_ylabel('Pixels per Flip')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Learning Progress (Loss)\n",
    "        ax3 = axes[1, 0]\n",
    "        if self.losses:\n",
    "            window = min(100, len(self.losses))\n",
    "            rolling_loss = np.convolve(self.losses, \n",
    "                                     np.ones(window)/window, mode='valid')\n",
    "            ax3.plot(range(len(rolling_loss)), rolling_loss, \n",
    "                    color='red', linewidth=1)\n",
    "            ax3.set_title('Training Loss (Rolling Average)')\n",
    "            ax3.set_xlabel('Training Step')\n",
    "            ax3.set_ylabel('Loss')\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Q-Value Analysis\n",
    "        ax4 = axes[1, 1]\n",
    "        if self.q_value_gaps:\n",
    "            ax4.plot(range(len(self.q_value_gaps)), self.q_value_gaps, \n",
    "                    color='purple', alpha=0.6)\n",
    "            ax4.set_title('Q-Value Gap (|Q(flip) - Q(wait)|)')\n",
    "            ax4.set_xlabel('Episode')\n",
    "            ax4.set_ylabel('Q-Value Difference')\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. Exploration Rate\n",
    "        ax5 = axes[2, 0]\n",
    "        ax5.plot(episodes, self.epsilons, color='orange', linewidth=2)\n",
    "        ax5.set_title('Exploration Rate (Epsilon)')\n",
    "        ax5.set_xlabel('Episode')\n",
    "        ax5.set_ylabel('Epsilon')\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Flips per Episode\n",
    "        ax6 = axes[2, 1]\n",
    "        ax6.plot(episodes, self.episode_flips, alpha=0.3, color='brown')\n",
    "        if len(self.episode_flips) >= 50:\n",
    "            rolling_flips = np.convolve(self.episode_flips, \n",
    "                                      np.ones(50)/50, mode='valid')\n",
    "            ax6.plot(range(49, len(episodes)), rolling_flips, \n",
    "                    color='brown', linewidth=2, label='50-episode average')\n",
    "        \n",
    "        ax6.set_title('Flips per Episode')\n",
    "        ax6.set_xlabel('Episode')\n",
    "        ax6.set_ylabel('Number of Flips')\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def train_improved_dqn(\n",
    "    agent,\n",
    "    n_episodes=500,\n",
    "    max_steps_per_episode=3600,  # 30 seconds at 120 FPS\n",
    "    print_every=10,\n",
    "    plot_every=100,\n",
    "    save_every=100,\n",
    "    eval_every=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the improved DQN agent with enhanced monitoring and evaluation\n",
    "    \"\"\"\n",
    "    # Create environment with improved rewards\n",
    "    env = GGEnv(\n",
    "        level_seed=None,        # Random levels\n",
    "        max_time_s=30.0,        # 30 second episodes\n",
    "        flip_penalty=0.1,       # Higher flip penalty\n",
    "        dt=1/120                # 120 FPS\n",
    "    )\n",
    "    \n",
    "    # Training monitor\n",
    "    monitor = ImprovedTrainingMonitor(window_size=100)\n",
    "    \n",
    "    print(\"üöÄ Starting Improved DQN Training!\")\n",
    "    print(f\"Episodes: {n_episodes}\")\n",
    "    print(f\"Target: Beat heuristic baseline of 2001px\")\n",
    "    print(f\"Environment: 30s episodes, random levels, 0.1 flip penalty\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for episode in range(1, n_episodes + 1):\n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        flips_count = 0\n",
    "        episode_loss = []\n",
    "        \n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Agent selects action\n",
    "            action = agent.act(state, training=True)\n",
    "            \n",
    "            # Take action in environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Calculate improved reward\n",
    "            progress = info.get('distance_delta', 0)\n",
    "            did_flip = info.get('did_flip', False)\n",
    "            \n",
    "            # Adjust reward based on efficiency\n",
    "            if did_flip:\n",
    "                flips_count += 1\n",
    "                efficiency = progress / 50.0  # Expected progress per flip\n",
    "                flip_penalty = -1.0 * max(0, 1.0 - efficiency)\n",
    "                reward = 0.1 * progress + flip_penalty\n",
    "            else:\n",
    "                reward = 0.1 * progress\n",
    "            \n",
    "            # Agent learns from experience\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Store loss if available\n",
    "            if agent.losses:\n",
    "                episode_loss.append(agent.losses[-1])\n",
    "            \n",
    "            # Update state and reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Record episode results\n",
    "        distance = info.get('distance_px', 0)\n",
    "        avg_loss = np.mean(episode_loss) if episode_loss else None\n",
    "        \n",
    "        # Get Q-values for a test state\n",
    "        test_state = torch.tensor([0.5, 0, 1, 1, 1, 0.8, 0.9, 1.0], \n",
    "                                dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = agent.q_network(test_state)[0].numpy()\n",
    "        \n",
    "        monitor.update(\n",
    "            episode=episode,\n",
    "            reward=total_reward,\n",
    "            distance=distance,\n",
    "            flips=flips_count,\n",
    "            length=steps,\n",
    "            epsilon=agent.epsilon,\n",
    "            loss=avg_loss,\n",
    "            q_values=q_values\n",
    "        )\n",
    "        \n",
    "        # Print progress\n",
    "        if episode % print_every == 0:\n",
    "            monitor.print_progress(episode, verbose=True)\n",
    "        \n",
    "        # Plot progress\n",
    "        if episode % plot_every == 0:\n",
    "            print(f\"\\nüìä Training Progress at Episode {episode}\")\n",
    "            monitor.plot_training()\n",
    "            \n",
    "            # Performance summary\n",
    "            avg_reward, avg_distance, avg_efficiency = monitor.get_averages()\n",
    "            print(f\"Current 100-episode averages:\")\n",
    "            print(f\"   Distance: {avg_distance:.0f}px\")\n",
    "            print(f\"   Efficiency: {avg_efficiency:.0f} px/flip\")\n",
    "            \n",
    "            # Compare to baselines\n",
    "            if avg_distance > 1576:\n",
    "                improvement = (avg_distance - 1576) / 1576 * 100\n",
    "                print(f\"‚úÖ Beating random by {improvement:.1f}%!\")\n",
    "            \n",
    "            if avg_distance > 2001:\n",
    "                improvement = (avg_distance - 2001) / 2001 * 100\n",
    "                print(f\"üéâ BEATING HEURISTIC by {improvement:.1f}%!\")\n",
    "            \n",
    "            print(\"-\" * 60)\n",
    "        \n",
    "        # Save model periodically\n",
    "        if episode % save_every == 0:\n",
    "            agent.save(f'./models/dqn/improved_dqn_episode_{episode}.pth')\n",
    "    \n",
    "    # Training complete!\n",
    "    elapsed_time = time.time() - start_time\n",
    "    final_reward, final_distance, final_efficiency = monitor.get_averages()\n",
    "    \n",
    "    print(f\"\\nüèÅ Training Complete!\")\n",
    "    print(f\"Total time: {elapsed_time/60:.1f} minutes\")\n",
    "    print(f\"Final 100-episode averages:\")\n",
    "    print(f\"   Distance: {final_distance:.0f}px\")\n",
    "    print(f\"   Efficiency: {final_efficiency:.0f} px/flip\")\n",
    "    \n",
    "    # Final comparison to baselines\n",
    "    print(f\"\\nüìä Final Performance Comparison:\")\n",
    "    print(f\"Random baseline:    1,576px\")\n",
    "    print(f\"Heuristic baseline: 2,001px\")\n",
    "    print(f\"DQN agent:         {final_distance:.0f}px\")\n",
    "    \n",
    "    if final_distance > 2001:\n",
    "        improvement = (final_distance - 2001) / 2001 * 100\n",
    "        print(f\"üéâ SUCCESS! Beat heuristic by {improvement:.1f}%\")\n",
    "    elif final_distance > 1576:\n",
    "        improvement = (final_distance - 1576) / 1576 * 100\n",
    "        print(f\"‚úÖ Good progress! Beat random by {improvement:.1f}%\")\n",
    "    else:\n",
    "        print(f\"üìà Keep training - agent needs more episodes\")\n",
    "    \n",
    "    # Save final model\n",
    "    agent.save('improved_dqn_final.pth')\n",
    "    print(f\"\\nüíæ Final model saved as 'improved_dqn_final.pth'\")\n",
    "    \n",
    "    # Final visualization\n",
    "    print(f\"\\nüìà Final Training Visualization:\")\n",
    "    monitor.plot_training()\n",
    "    \n",
    "    return monitor\n",
    "\n",
    "# Create and train the improved agent\n",
    "print(\"=== STARTING IMPROVED DQN TRAINING ===\")\n",
    "print(\"This will take 20-25 minutes to train for 500 episodes.\")\n",
    "print(\"You'll see much more detailed progress updates and metrics.\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Training run\n",
    "training_monitor = train_improved_dqn(\n",
    "    agent=improved_agent,\n",
    "    n_episodes=500,              # Full training run\n",
    "    max_steps_per_episode=3600,  # 30 seconds at 120 FPS\n",
    "    print_every=10,              # Frequent updates\n",
    "    plot_every=100,              # Regular plotting\n",
    "    save_every=100               # Regular checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fb0995",
   "metadata": {},
   "source": [
    "### Understanding the Training Process\n",
    "\n",
    "**üß† What Happens During Training:**\n",
    "\n",
    "1. **Episode Start**: Agent spawns in random level, starts with high exploration (95%)\n",
    "2. **Action Selection**: Agent chooses actions (mostly random at first, gradually more intelligent)\n",
    "3. **Experience Storage**: Every action/reward/outcome gets stored in memory\n",
    "4. **Learning**: Agent trains on random batches from its memory\n",
    "5. **Progress**: Over time, exploration decreases and performance improves\n",
    "\n",
    "**üìä What to Watch For:**\n",
    "\n",
    "- **Distance Plot**: Should gradually increase from ~1576px (random) toward 2001px+ (better than heuristic)\n",
    "- **Exploration Decay**: Epsilon should smoothly decrease from 0.95 to 0.05\n",
    "- **Flip Efficiency**: Distance per flip should improve as agent learns better timing\n",
    "- **Stability**: Less variance in performance as training progresses\n",
    "\n",
    "**üéØ Success Metrics:**\n",
    "\n",
    "- **Beat Random**: Consistently above 1576px average distance\n",
    "- **Beat Heuristic**: Consistently above 2001px average distance  \n",
    "- **Efficiency**: Fewer flips per distance than random baseline\n",
    "- **Consistency**: Lower variance in episode performance\n",
    "\n",
    "The agent should start performing like your random baseline but gradually learn to match or exceed your heuristic baseline of 2001px!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7274a8c",
   "metadata": {},
   "source": [
    "# Execute this cell for full training (after quick test succeeds)\n",
    "\n",
    "## Full DQN Training Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a439212b",
   "metadata": {},
   "source": [
    "# Improved DQN Training with Enhanced State\n",
    "\n",
    "Our improved DQN agent needs to handle the 6-dimensional state from the environment plus the 2 extra dimensions we get from the info dict:\n",
    "\n",
    "1. Original State (6D):\n",
    "   - y_norm ‚àà [0,1]\n",
    "   - vy_norm ‚àà [-1,1]\n",
    "   - grav_dir ‚àà {‚àí1,+1}\n",
    "   - probes[0,1,2] ‚àà [0,1]\n",
    "\n",
    "2. Extra Info (2D):\n",
    "   - grounded (bool)\n",
    "   - cooldown_ready (bool)\n",
    "\n",
    "The training code will combine these into a full 8D state vector before passing it to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "05a1f023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ DQN Agent Created:\n",
      "   State size: 8\n",
      "   Hidden size: 128\n",
      "   Learning rate: 0.0001\n",
      "   Batch size: 32\n",
      "\n",
      "‚úÖ Agent created and ready for training!\n"
     ]
    }
   ],
   "source": [
    "class StatePreprocessor:\n",
    "    \"\"\"Handles state preprocessing and augmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.base_dim = 6  # y, vy, grav_dir, 3 probes\n",
    "    \n",
    "    def process_state(self, state, info):\n",
    "        \"\"\"Convert 6D state + info into 8D state vector\"\"\"\n",
    "        if isinstance(state, torch.Tensor):\n",
    "            state = state.numpy()\n",
    "        state = np.array(state).flatten()\n",
    "        if len(state) != self.base_dim:\n",
    "            raise ValueError(f\"Expected state dimension {self.base_dim}, got {len(state)}\")\n",
    "        \n",
    "        grounded = float(info.get('grounded', False))\n",
    "        cooldown_ready = float(info.get('cooldown', 0) <= 0)\n",
    "        return np.concatenate([state, [grounded, cooldown_ready]])\n",
    "    \n",
    "    def process_batch(self, states, infos):\n",
    "        \"\"\"Process a batch of states\"\"\"\n",
    "        return np.array([self.process_state(s, i) for s, i in zip(states, infos)])\n",
    "    \n",
    "    def get_state_size(self):\n",
    "        \"\"\"Return size of processed state\"\"\"\n",
    "        return self.base_dim + 2  # 6 base dims + 2 extra\n",
    "\n",
    "class DQNNetwork(nn.Module):\n",
    "    \"\"\"Neural network for DQN with batch normalization\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.BatchNorm1d(hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, action_size)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.unsqueeze(0)  # Add batch dimension\n",
    "        return self.net(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer with uniform sampling\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards),\n",
    "            np.array(next_states),\n",
    "            np.array(dones)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent with state preprocessing and enhanced training\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_processor,\n",
    "        action_size=2,\n",
    "        hidden_size=128,\n",
    "        learning_rate=0.0001,\n",
    "        gamma=0.99,\n",
    "        epsilon_start=0.99,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=0.9999,\n",
    "        buffer_size=100000,\n",
    "        batch_size=32,\n",
    "        target_update_freq=100\n",
    "    ):\n",
    "        self.state_processor = state_processor\n",
    "        self.state_size = state_processor.get_state_size()\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.target_update_freq = target_update_freq\n",
    "        \n",
    "        # Exploration\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        # Networks\n",
    "        self.q_network = DQNNetwork(self.state_size, action_size, hidden_size)\n",
    "        self.target_network = DQNNetwork(self.state_size, action_size, hidden_size)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        # Training stats\n",
    "        self.steps_done = 0\n",
    "        self.losses = []\n",
    "        \n",
    "        print(f\"ü§ñ DQN Agent Created:\")\n",
    "        print(f\"   State size: {self.state_size}\")\n",
    "        print(f\"   Hidden size: {hidden_size}\")\n",
    "        print(f\"   Learning rate: {learning_rate}\")\n",
    "        print(f\"   Batch size: {batch_size}\")\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done, info, next_info):\n",
    "        \"\"\"Store experience in replay buffer\"\"\"\n",
    "        # Process states before storing\n",
    "        full_state = self.state_processor.process_state(state, info)\n",
    "        full_next_state = self.state_processor.process_state(next_state, next_info)\n",
    "        self.memory.push(full_state, action, reward, full_next_state, done)\n",
    "    \n",
    "    def get_action(self, state, info, training=True):\n",
    "        \"\"\"Select action using epsilon-greedy\"\"\"\n",
    "        # Process state\n",
    "        full_state = self.state_processor.process_state(state, info)\n",
    "        state_tensor = torch.FloatTensor(full_state)\n",
    "        \n",
    "        # Epsilon-greedy\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Train on one batch of experiences\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample batch\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Next Q values (with target network)\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_network(next_states).max(1)[0]\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "        \n",
    "        # Compute loss and update\n",
    "        loss = F.smooth_l1_loss(current_q.squeeze(), target_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Store loss\n",
    "        self.losses.append(loss.item())\n",
    "        \n",
    "        # Update target network if needed\n",
    "        self.steps_done += 1\n",
    "        if self.steps_done % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save model\"\"\"\n",
    "        torch.save({\n",
    "            'q_network_state_dict': self.q_network.state_dict(),\n",
    "            'target_network_state_dict': self.target_network.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon,\n",
    "            'steps_done': self.steps_done,\n",
    "            'losses': self.losses\n",
    "        }, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load model\"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "        self.steps_done = checkpoint['steps_done']\n",
    "        self.losses = checkpoint['losses']\n",
    "\n",
    "# Create environment and agent\n",
    "env = GGEnv(\n",
    "    level_seed=None,\n",
    "    max_time_s=30.0,\n",
    "    flip_penalty=0.1,\n",
    "    dt=1/120\n",
    ")\n",
    "\n",
    "# Create state processor and agent\n",
    "state_processor = StatePreprocessor(env)\n",
    "agent = DQNAgent(\n",
    "    state_processor=state_processor,\n",
    "    action_size=2,\n",
    "    hidden_size=128,\n",
    "    learning_rate=0.0001,\n",
    "    gamma=0.99,\n",
    "    epsilon_start=0.99,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=0.9999,\n",
    "    buffer_size=100000,\n",
    "    batch_size=32,\n",
    "    target_update_freq=100\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Agent created and ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a171f3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéÆ Starting training...\n",
      "\n",
      "‚ùå Training interrupted: too many values to unpack (expected 2)\n",
      "\n",
      "üéâ Training complete!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[1;32m---> 15\u001b[0m         state, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m     16\u001b[0m         episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     17\u001b[0m         episode_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "num_episodes = 1000\n",
    "max_steps = 1000\n",
    "log_interval = 10\n",
    "save_interval = 100\n",
    "\n",
    "# Training metrics\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "avg_losses = []\n",
    "\n",
    "print(\"üéÆ Starting training...\")\n",
    "try:\n",
    "    for episode in range(num_episodes):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Select and perform action\n",
    "            action = agent.get_action(state, info, training=True)\n",
    "            next_state, reward, done, next_info = env.step(action)\n",
    "            \n",
    "            # Store in memory (with proper state processing)\n",
    "            agent.remember(state, action, reward, next_state, done, info, next_info)\n",
    "            \n",
    "            # Train\n",
    "            agent.train_step()\n",
    "            \n",
    "            # Update metrics\n",
    "            episode_reward += reward\n",
    "            episode_steps += 1\n",
    "            \n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            info = next_info\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Store episode metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(episode_steps)\n",
    "        if agent.losses:\n",
    "            avg_losses.append(np.mean(agent.losses[-episode_steps:]))\n",
    "        \n",
    "        # Logging\n",
    "        if (episode + 1) % log_interval == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-log_interval:])\n",
    "            avg_length = np.mean(episode_lengths[-log_interval:])\n",
    "            avg_loss = np.mean(avg_losses[-log_interval:]) if avg_losses else 0\n",
    "            print(f\"\\nEpisode {episode + 1}\")\n",
    "            print(f\"Average Reward: {avg_reward:.2f}\")\n",
    "            print(f\"Average Length: {avg_length:.2f}\")\n",
    "            print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "            print(f\"Epsilon: {agent.epsilon:.4f}\")\n",
    "        \n",
    "        # Save model\n",
    "        if (episode + 1) % save_interval == 0:\n",
    "            agent.save(f\"models/dqn_ep{episode + 1}.pt\")\n",
    "            print(f\"\\nüíæ Model saved at episode {episode + 1}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training interrupted: {str(e)}\")\n",
    "    raise\n",
    "finally:\n",
    "    # Plot training results\n",
    "    if len(episode_rewards) > 0:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(episode_rewards)\n",
    "        plt.title('Episode Rewards')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(episode_lengths)\n",
    "        plt.title('Episode Lengths')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Steps')\n",
    "        \n",
    "        if avg_losses:\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.plot(avg_losses)\n",
    "            plt.title('Average Loss')\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Loss')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\nüéâ Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
