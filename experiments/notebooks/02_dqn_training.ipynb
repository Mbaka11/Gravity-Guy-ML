{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6701996d",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN) for Gravity-Guy Env v2 — Plan & Rationale\n",
    "\n",
    "**Goal.** Train a DQN agent that outperforms both **Random** and our **Tiny Heuristic** on a held-out set of seeds.  \n",
    "We’ll keep the notebook educational: each section explains *what we do* and *why*, before showing code.\n",
    "\n",
    "**Notebook roadmap**\n",
    "1. **DQN at a glance (this section):** problem framing & core equations (high-level, with formulas).\n",
    "2. **Experiment setup:** action/obs spaces, reward, time limits, seeds, evaluation protocol, logging.\n",
    "3. **Network & optimizer choices:** architecture, activations, initialization, loss, target updates, exploration.\n",
    "4. **Training loop design:** replay buffer, batches, update cadence, target sync, eval cadence, checkpoints.\n",
    "5. **Implementation:** minimal training code, clean metrics.\n",
    "6. **Results & analysis:** curves, tables, seed-paired eval, side-by-side with heuristic.\n",
    "7. **Next steps:** ablations (Double DQN, PER, n-step) and improvements.\n",
    "\n",
    "*Environment:* GGEnv v2, Observation v2 (15-dim), discrete actions (2), default decision rate ≈ 15 Hz (`frame_skip=4`).\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1034041",
   "metadata": {},
   "source": [
    "## Part 1 — DQN at a glance\n",
    "\n",
    "### 1) Problem framing (MDP)\n",
    "We model the game as a Markov Decision Process (MDP):\n",
    "- **State** $s_t$: the 15-dim observation vector (player position/velocity/gravity + probe features).\n",
    "- **Action** $a_t \\in \\{0,1\\}$: `0 = NOOP`, `1 = FLIP` gravity.\n",
    "- **Reward** $r_t$: scalar signal per step (defined in the setup section).\n",
    "- **Transition**: environment moves platforms, applies gravity, checks collisions → $s_{t+1}$.\n",
    "- **Discount** $\\gamma \\in [0,1)$: how much we value future rewards.\n",
    "\n",
    "The objective is to learn a policy $\\pi(a \\mid s)$ that maximizes expected discounted return.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Q-learning objective\n",
    "The **optimal action-value** function $Q^*(s,a)$ satisfies the Bellman optimality equation:\n",
    "\n",
    "$$\n",
    "Q^*(s_t,a_t) = \\mathbb{E}\\big[r_t + \\gamma \\max_{a'} Q^*(s_{t+1}, a') \\,\\big]\n",
    "$$\n",
    "\n",
    "DQN approximates $Q(s,a;\\theta)$ with a neural network and minimizes a temporal-difference (TD) loss toward a **target**:\n",
    "\n",
    "$$\n",
    "y_t = r_t + \\gamma (1 - \\text{done}_t) \\, \\max_{a'} Q(s_{t+1}, a'; \\theta^-)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\mathbb{E}\\left[\\, \\ell\\big(y_t - Q(s_t, a_t; \\theta)\\big) \\,\\right]\n",
    "$$\n",
    "\n",
    "- $\\theta$: online network parameters (updated every gradient step).  \n",
    "- $\\theta^-$: **target network** parameters (held fixed; periodically or softly synced from $\\theta$).  \n",
    "- $\\ell(\\cdot)$: loss; we’ll use **Huber** (smooth L1) for stability:\n",
    "\n",
    "$$\n",
    "\\ell(\\delta)=\n",
    "\\begin{cases}\n",
    "\\tfrac{1}{2}\\delta^2 & \\text{if } |\\delta|\\le \\kappa \\\\\n",
    "\\kappa\\,(|\\delta| - \\tfrac{1}{2}\\kappa) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "with $\\kappa=1$ by default. (MSE also works but is less robust to outliers.)\n",
    "\n",
    "> **Double DQN (optional):** reduces over-estimation by selecting with $\\theta$ but evaluating with $\\theta^-$:\n",
    "> $$\n",
    "> y_t = r_t + \\gamma (1-\\text{done}_t)\\, Q\\!\\left(s_{t+1}, \\arg\\max_{a'} Q(s_{t+1},a';\\theta);\\ \\theta^- \\right)\n",
    "> $$\n",
    "> We’ll start with vanilla DQN and can switch to Double DQN if needed.\n",
    "\n",
    "---\n",
    "\n",
    "### 3) Experience replay & exploration\n",
    "- **Replay buffer** $\\mathcal{D}$: stores transitions $(s_t,a_t,r_t,s_{t+1},\\text{done})$.  \n",
    "  At each update, we sample a mini-batch **i.i.d.** from $\\mathcal{D}$ to decorrelate updates and stabilize training.\n",
    "- **$\\varepsilon$-greedy exploration**: with prob. $\\varepsilon_t$ choose a random action; otherwise act greedily:\n",
    "\n",
    "$$\n",
    "a_t =\n",
    "\\begin{cases}\n",
    "\\text{rand action} & \\text{with prob } \\varepsilon_t \\\\\n",
    "\\arg\\max_a Q(s_t,a;\\theta) & \\text{with prob } 1-\\varepsilon_t\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We’ll **anneal** $\\varepsilon$ linearly from 1.0 to 0.05 over a fixed number of decision steps.\n",
    "\n",
    "---\n",
    "\n",
    "### 4) Network & activation (why ReLU?)\n",
    "- **Input:** 15-dim observation (already normalized/scaled by design).\n",
    "- **Output:** 2 Q-values $[Q(s,NOOP),\\,Q(s,FLIP)]$.\n",
    "- **Backbone:** MLP with two hidden layers (e.g., **256 → 256**, **ReLU**).\n",
    "  - **Why ReLU?** Simple, fast, avoids vanishing gradients, works well on sparse/tabular-like signals (our probes).\n",
    "  - We’ll use **Xavier/He** initialization (framework defaults) and **Adam** optimizer (good adaptive steps).\n",
    "\n",
    "---\n",
    "\n",
    "### 5) Target updates & stability knobs\n",
    "- **Target network sync:** either **hard** copy every $C$ updates or **soft** update  \n",
    "  $\\theta^- \\leftarrow \\tau \\theta + (1-\\tau)\\theta^-$ with small $\\tau$.\n",
    "- **Gradient clipping:** cap global norm (e.g., 10) to prevent rare exploding updates.\n",
    "- **Reward clipping (optional):** clip $r_t \\in [-1,1]$ if rewards are unbounded.  \n",
    "- **Batch size / buffer size:** large enough to decorrelate; we’ll start with batch 256, buffer 100k.\n",
    "\n",
    "---\n",
    "\n",
    "### 6) What “success” looks like here\n",
    "- On held-out seeds, DQN surpasses both **Random** and **Tiny Heuristic** in:\n",
    "  - **distance** (px), **episode length** (s),\n",
    "  - **death-cause mix** (fewer early spikes *and* fewer OOB),\n",
    "  - learning curves that improve steadily without divergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd7751",
   "metadata": {},
   "source": [
    "## Part 2 — Experiment setup & evaluation protocol\n",
    "\n",
    "**Goal.** Fix all the “contract” details before writing any training code so results are reproducible and comparable.\n",
    "\n",
    "### 1) Environment contract\n",
    "- **Env:** `GGEnv v2` (Observation v2, 15-dim).\n",
    "- **Actions:** discrete {0 = NOOP, 1 = FLIP}.\n",
    "- **Decision cadence:** `frame_skip = 4` ⇒ ~15 decisions/sec at 60 FPS physics.\n",
    "- **Time limit:** 30 s simulated per episode ⇒ **max_steps ≈ 450** decisions.\n",
    "- **Seeding:** full determinism per episode seed (and per-run RNG seeds).\n",
    "\n",
    "> We’ll keep the env reward as-is (alive reward per decision, small terminal penalty if defined).  \n",
    "> Our *primary* evaluation metrics remain **distance (px)** and **episode length (s)**.\n",
    "\n",
    "### 2) Datasets: training vs evaluation seeds\n",
    "- **Eval seeds (held-out):** reuse the 20 seeds from the sanity notebook (101–120).  \n",
    "  We never train on these; we only evaluate on them periodically and at the end.\n",
    "- **Training seeds:** a larger pool (e.g., 1000–1999) sampled per episode to avoid overfitting.\n",
    "\n",
    "### 3) Logging & folders (so replay works)\n",
    "We’ll log under `experiments/runs/dqn/<run_id>/`:\n",
    "- `config.json` — all hyperparams & seeds\n",
    "- `metrics.csv` — rolling train stats (loss, eps, steps, buffer size, etc.)\n",
    "- `eval/episodes.csv` — eval summaries on the held-out seeds\n",
    "- `eval/traces/<seed>_actions.npy` — action sequences for exact replay\n",
    "- `checkpoints/` — network snapshots (e.g., best and periodic)\n",
    "\n",
    "### 4) Hyperparameters (first pass; we’ll freeze them in Part 3)\n",
    "- Network: MLP 2×256, ReLU, output dimension = 2 Q-values\n",
    "- Optimizer: Adam (lr = 1e-3), Huber loss, γ = 0.99\n",
    "- Replay buffer: 100k, batch = 256\n",
    "- Train every 4 decisions; target sync every 1,000 updates\n",
    "- ε-greedy: 1.0 → 0.05 linearly over ~100k decisions\n",
    "\n",
    "We’ll confirm/adjust these after a small smoke run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b05b21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to sys.path: D:\\Projects\\GravityGuyML\n",
      "Created D:\\Projects\\GravityGuyML\\src\\__init__.py\n",
      "Import OK: <class 'src.env.gg_env_v2.GGEnv'>\n"
     ]
    }
   ],
   "source": [
    "# Make the repo root (the folder that contains `src/`) importable\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Start from the current notebook dir and walk upwards until we find a 'src' folder\n",
    "here = Path.cwd().resolve()\n",
    "repo_root = None\n",
    "for parent in [here, *here.parents]:\n",
    "    if (parent / \"src\").exists():\n",
    "        repo_root = parent\n",
    "        break\n",
    "\n",
    "if repo_root is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Couldn't find a 'src' directory by walking up from {here}. \"\n",
    "        \"Open this notebook from inside your repo or adjust the path below manually.\"\n",
    "    )\n",
    "\n",
    "# Put repo root at the front of sys.path\n",
    "sys.path.insert(0, str(repo_root))\n",
    "print(\"Added to sys.path:\", repo_root)\n",
    "\n",
    "# Optional: ensure packages are recognized (create empty __init__.py if missing)\n",
    "for pkg in [\"src\", \"src/env\", \"src/game\"]:\n",
    "    init = repo_root / pkg / \"__init__.py\"\n",
    "    if not init.exists():\n",
    "        try:\n",
    "            init.touch()\n",
    "            print(\"Created\", init)\n",
    "        except Exception as e:\n",
    "            print(\"Note:\", init, \"does not exist and couldn't be created automatically:\", e)\n",
    "\n",
    "# Sanity import\n",
    "from src.env.gg_env_v2 import GGEnv\n",
    "print(\"Import OK:\", GGEnv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "387ad80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run dir: d:\\Projects\\GravityGuyML\\experiments\\runs\\dqn\\20250911_015257\n",
      "Decision rate ≈ 15.0 Hz; max_steps = 450\n",
      "Eval seeds (20): 101..120\n",
      "Train seed range: [1000, 2000)\n",
      "Obs space: Box([ 0. -1. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.], 1.0, (15,), float32)\n",
      "Act space: Discrete(2)\n",
      "Wrote: d:\\Projects\\GravityGuyML\\experiments\\runs\\dqn\\20250911_015257\\config.json\n"
     ]
    }
   ],
   "source": [
    "import os, random, json, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from src.env.gg_env_v2 import GGEnv\n",
    "\n",
    "# ----- Paths (assuming notebook in experiments/notebooks/) -----\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "EXP_DIR = NOTEBOOK_DIR.parent                   # experiments/\n",
    "RUNS_BASE = EXP_DIR / \"runs\" / \"dqn\"            # experiments/runs/dqn/\n",
    "RUNS_BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Unique run id (timestamp) and run directory\n",
    "RUN_ID = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_DIR = RUNS_BASE / RUN_ID\n",
    "(RUN_DIR / \"eval\" / \"traces\").mkdir(parents=True, exist_ok=True)\n",
    "(RUN_DIR / \"checkpoints\").mkdir(parents=True, exist_ok=True)\n",
    "print(\"Run dir:\", RUN_DIR)\n",
    "\n",
    "# ----- Core env settings -----\n",
    "SIM_FPS = 60\n",
    "FRAME_SKIP = 4\n",
    "DECISION_HZ = SIM_FPS / FRAME_SKIP\n",
    "TIME_LIMIT_S = 30\n",
    "MAX_STEPS = int(TIME_LIMIT_S * DECISION_HZ)\n",
    "\n",
    "# ----- Seed sets -----\n",
    "EVAL_SEEDS = list(range(101, 121))               # held-out\n",
    "TRAIN_SEED_RANGE = (1000, 2000)                  # sampled per episode\n",
    "\n",
    "# ----- Global RNG seeds for reproducibility (you can change these) -----\n",
    "GLOBAL_SEED = 42\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "random.seed(GLOBAL_SEED)\n",
    "\n",
    "print(f\"Decision rate ≈ {DECISION_HZ:.1f} Hz; max_steps = {MAX_STEPS}\")\n",
    "print(f\"Eval seeds ({len(EVAL_SEEDS)}): {EVAL_SEEDS[0]}..{EVAL_SEEDS[-1]}\")\n",
    "print(f\"Train seed range: [{TRAIN_SEED_RANGE[0]}, {TRAIN_SEED_RANGE[1]})\")\n",
    "\n",
    "# ----- Small helper: env factory -----\n",
    "def make_env(seed: int, render_mode=None):\n",
    "    \"\"\"\n",
    "    Create a GGEnv v2 with the agreed contract.\n",
    "    We keep render_mode=None for training and 'human' for debug runs.\n",
    "    \"\"\"\n",
    "    env = GGEnv(render_mode=render_mode, frame_skip=FRAME_SKIP)\n",
    "    # Let env carry its own time limit; if you need a hard cap, wrap with gym.wrappers.TimeLimit\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    return env\n",
    "\n",
    "# Sanity: instantiate once headless (no window), check spaces\n",
    "_env = make_env(seed=EVAL_SEEDS[0], render_mode=None)\n",
    "print(\"Obs space:\", _env.observation_space)\n",
    "print(\"Act space:\", _env.action_space)\n",
    "_env.close()\n",
    "\n",
    "# Save a tiny config snapshot for the run folder now\n",
    "config_snapshot = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"sim_fps\": SIM_FPS,\n",
    "    \"frame_skip\": FRAME_SKIP,\n",
    "    \"decision_hz\": DECISION_HZ,\n",
    "    \"time_limit_s\": TIME_LIMIT_S,\n",
    "    \"max_steps\": MAX_STEPS,\n",
    "    \"eval_seeds\": EVAL_SEEDS,\n",
    "    \"train_seed_range\": TRAIN_SEED_RANGE,\n",
    "    \"global_seed\": GLOBAL_SEED,\n",
    "}\n",
    "with (RUN_DIR / \"config.json\").open(\"w\") as f:\n",
    "    json.dump(config_snapshot, f, indent=2)\n",
    "print(\"Wrote:\", RUN_DIR / \"config.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe736a",
   "metadata": {},
   "source": [
    "## Part 3 — Network & optimizer choices\n",
    "\n",
    "### Architecture (DQN head)\n",
    "- **Input:** 15-dim observation (obs v2).\n",
    "- **Output:** 2 Q-values $[Q(s,\\text{NOOP}),\\, Q(s,\\text{FLIP})]$.\n",
    "- **Backbone:** MLP with two hidden layers: **256 → 256**, **ReLU** activations.\n",
    "\n",
    "**Why ReLU?**  \n",
    "Simple, fast, robust to vanishing gradients, and works well on sparse/tabular-ish inputs (our probe features). PyTorch’s default Kaiming/He initialization is designed for ReLU.\n",
    "\n",
    "### Loss & targets\n",
    "- **TD target:** $ y_t = r_t + \\gamma (1 - \\text{done}_t)\\max_{a'} Q(s_{t+1}, a'; \\theta^-) $\n",
    "- **Loss:** **Huber (smooth L1)** on $(y_t - Q(s_t,a_t;\\theta))$ for outlier robustness.\n",
    "- **Target network:** copy online → target every **C = 1000** updates (hard sync).  \n",
    "  *(We can switch to soft updates or Double DQN later if needed.)*\n",
    "\n",
    "### Optimizer & stability\n",
    "- **Optimizer:** Adam, **lr = 1e-3**.\n",
    "- **Discount:** $\\gamma = 0.99$.\n",
    "- **Gradient clipping:** global-norm cap **10.0** to avoid rare spikes.\n",
    "- **Replay buffer:** **100k** transitions; **batch = 256**; train every **4** decisions.\n",
    "- **Warmup:** collect **5k** decisions with pure exploration before first gradient step.\n",
    "\n",
    "### Exploration (ε-greedy)\n",
    "- Start **ε=1.0**, linearly anneal to **ε=0.05** over **100k** decisions:\n",
    "\n",
    "$$\n",
    "\\varepsilon_t = \\max\\big(\\varepsilon_{\\min}, \\varepsilon_{\\max} - (\\varepsilon_{\\max}-\\varepsilon_{\\min}) \\cdot t / T\\big)\n",
    "$$\n",
    "\n",
    "where $T = 100{,}000$ decisions.\n",
    "\n",
    "### What success should look like\n",
    "- On held-out seeds, the trained DQN surpasses **Random** and the **Tiny Heuristic** in **distance** and **episode length**, and reduces both **early spikes** and **OOB** deaths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95345c24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
