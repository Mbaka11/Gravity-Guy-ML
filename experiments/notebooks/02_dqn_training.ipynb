{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87cf55c0",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN) for Gravity Guy\n",
    "\n",
    "## What is DQN?\n",
    "**Deep Q-Network (DQN)** is a reinforcement learning method that learns a function\n",
    "$Q_\\theta(s,a)$ estimating the long-term return of taking action $a$ in state $s$.\n",
    "We act with **ε-greedy** (mostly pick the action with the highest Q, sometimes explore),\n",
    "and we **train** the network to match a bootstrapped target:\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "r & \\text{if episode terminated}\\\\\n",
    "r + \\gamma \\max_{a'} Q_{\\bar\\theta}(s', a') & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\mathrm{Huber}\\big( Q_\\theta(s,a) - y \\big)\n",
    "$$\n",
    "\n",
    "Two stabilizers make DQN work well in practice:\n",
    "- **Replay buffer**: learn from randomized past transitions $(s,a,r,s',\\text{done})$ to break correlations.\n",
    "- **Target network** $Q_{\\bar\\theta}$: a slowly updated copy used to compute $y$.\n",
    "\n",
    "## Why DQN fits this game\n",
    "- **Tiny, discrete action space:** 2 actions (NOOP / FLIP).\n",
    "- **Dense, shaped reward:** per-step progress minus a small flip penalty.\n",
    "- **Compact observation:** 6 floats capture what matters (vertical state + look-ahead probes).\n",
    "- **Fast, headless env:** high sample throughput for replay.\n",
    "\n",
    "## State / Action / Reward (this notebook)\n",
    "- **Observation (6 floats):**\n",
    "  1. `y_norm` ∈ [0,1] — vertical position (0=top, 1=bottom)  \n",
    "  2. `vy_norm` ∈ [-1,1] — normalized vertical speed  \n",
    "  3. `grav_dir` ∈ {−1,+1} — current gravity (up/down)  \n",
    "  4–6. `p1, p2, p3` ∈ [0,1] — **look-ahead clearances** in the gravity direction (near → far)\n",
    "- **Actions:** `0 = NOOP`, `1 = FLIP` (flip only **fires** when grounded & cooldown is over; invalid flips act as no-ops).\n",
    "- **Reward per step:** `progress − flip_penalty × [flip_fired]`\n",
    "- **Termination:** off-screen (death) or time limit (e.g., 10 s).\n",
    "\n",
    "## Learning loop (at a glance)\n",
    "1. **Observe** state $s$.  \n",
    "2. **Act** with ε-greedy: pick `argmax_a Q_\\theta(s,a)` with prob $1-ε$, random action with prob $ε$.  \n",
    "3. **Step** the env → get $(r, s', \\text{done})$.  \n",
    "4. **Store** $(s,a,r,s',\\text{done})$ in the replay buffer.  \n",
    "5. **Sample** a mini-batch from replay, compute targets $y$ with the **target network**.  \n",
    "6. **Update** the online network $Q_\\theta$ to minimize Huber loss; periodically **update** the target network.  \n",
    "7. **Anneal** $ε$ over time to reduce exploration.\n",
    "\n",
    "## Game-specific caveats (and how we handle them)\n",
    "- **Action validity:** flips only take effect when grounded.  \n",
    "  *Mitigation:* treat invalid flips as no-ops and/or mask them at action selection time.\n",
    "- **Timing & partial observability:** probes look ahead in x while y changes over time.  \n",
    "  *Mitigation:* keep the observation compact but informative (probes + gravity + velocity).  \n",
    "  (Optionally, stack a few recent observations or add `grounded`/`cooldown` scalars.)\n",
    "- **Evaluation fairness:** fix a set of level seeds; report mean/median distance, % time-limit terminations, and flips per 1000 px.\n",
    "\n",
    "## What the reader should expect\n",
    "- Baselines (**Random**, **Heuristic**) for context.  \n",
    "- A DQN agent that learns to time flips better than random, often matching or surpassing the hand-crafted heuristic on held-out seeds.  \n",
    "- Clear plots: training return, evaluation distance, and failure-mode breakdown.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4871d07b",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Let's start by setting up our environment and importing the libraries we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1169b30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "PyTorch version: 2.8.0+cpu\n",
      "Using device: CPU\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries for ML and data handling\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "# PyTorch for neural networks (you might need: pip install torch)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Our custom environment\n",
    "import sys\n",
    "sys.path.append('../..')  # Go up two directories to access src/\n",
    "from src.env.gg_env import GGEnv\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Using device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3881d4",
   "metadata": {},
   "source": [
    "## Quick Environment Test\n",
    "\n",
    "Before building our AI, let's make sure we understand our environment perfectly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e22f0a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENVIRONMENT UNDERSTANDING ===\n",
      "Observation space: 6 dimensions\n",
      "Action space: 2 actions (0=wait, 1=flip)\n",
      "First observation: [0.5, 0.0, 1.0, 0.24814814814814815, 0.24814814814814815, 0.24814814814814815]\n",
      "Step 1: action=0, reward=2.083, done=False\n",
      "  → obs: [0.50, 0.01, 1, 0.25, 0.25, 0.25]\n",
      "Step 2: action=0, reward=2.083, done=False\n",
      "  → obs: [0.50, 0.03, 1, 0.25, 0.25, 0.25]\n",
      "Step 3: action=0, reward=2.083, done=False\n",
      "  → obs: [0.50, 0.04, 1, 0.25, 0.25, 0.25]\n",
      "Step 4: action=0, reward=2.083, done=False\n",
      "  → obs: [0.50, 0.05, 1, 0.25, 0.25, 0.25]\n",
      "Step 5: action=0, reward=2.083, done=False\n",
      "  → obs: [0.50, 0.06, 1, 0.25, 0.25, 0.25]\n",
      "Step 6: action=0, reward=2.083, done=False\n",
      "  → obs: [0.51, 0.07, 1, 0.24, 0.24, 0.24]\n",
      "Step 7: action=1, reward=2.083, done=False\n",
      "  → obs: [0.51, 0.09, 1, 0.24, 0.24, 0.24]\n",
      "Step 8: action=1, reward=2.083, done=False\n",
      "  → obs: [0.51, 0.10, 1, 0.24, 0.24, 0.24]\n",
      "Step 9: action=0, reward=2.083, done=False\n",
      "  → obs: [0.51, 0.11, 1, 0.24, 0.24, 0.24]\n",
      "Step 10: action=1, reward=2.083, done=False\n",
      "  → obs: [0.51, 0.12, 1, 0.24, 0.24, 0.24]\n",
      "\n",
      "✅ Environment test completed!\n"
     ]
    }
   ],
   "source": [
    "# Create a test environment\n",
    "env = GGEnv(level_seed=12345, max_time_s=10.0, flip_penalty=0.01)\n",
    "obs = env.reset()\n",
    "\n",
    "print(\"=== ENVIRONMENT UNDERSTANDING ===\")\n",
    "print(f\"Observation space: {len(obs)} dimensions\")\n",
    "print(f\"Action space: {env.action_space_n} actions (0=wait, 1=flip)\")\n",
    "print(f\"First observation: {obs}\")\n",
    "\n",
    "# Take a few random actions to see what happens\n",
    "total_reward = 0\n",
    "for step in range(10):\n",
    "    action = random.choice([0, 1])  # Random action\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    print(f\"Step {step+1}: action={action}, reward={reward:.3f}, done={done}\")\n",
    "    print(f\"  → obs: [{obs[0]:.2f}, {obs[1]:.2f}, {obs[2]:.0f}, {obs[3]:.2f}, {obs[4]:.2f}, {obs[5]:.2f}]\")\n",
    "    \n",
    "    if done:\n",
    "        print(f\"Episode ended! Total reward: {total_reward:.2f}, Distance: {info['distance_px']}px\")\n",
    "        break\n",
    "\n",
    "print(\"\\n✅ Environment test completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c718570",
   "metadata": {},
   "source": [
    "## Part 2: Building the Neural Network Brain\n",
    "\n",
    "### What is our Neural Network doing?\n",
    "\n",
    "Think of the neural network as the agent's \"brain\". It takes in the 6 observations from the game and outputs 2 numbers:\n",
    "- **Q(state, wait)**: How good is it to wait/do nothing in this situation?\n",
    "- **Q(state, flip)**: How good is it to flip gravity in this situation?\n",
    "\n",
    "The agent will always choose the action with the higher Q-value.\n",
    "\n",
    "### Network Architecture Design\n",
    "\n",
    "For our Gravity Guy game, we'll use a simple but effective architecture:\n",
    "\n",
    "```\n",
    "Input Layer (6 neurons) → Hidden Layer (128 neurons) → Hidden Layer (64 neurons) → Output Layer (2 neurons)\n",
    "       ↓                        ↓                           ↓                        ↓\n",
    "  [y, vy, grav,              [lots of                   [more                   [Q(wait), \n",
    "   p1, p2, p3]                neurons]                   neurons]                 Q(flip)]\n",
    "```\n",
    "\n",
    "### Why this architecture?\n",
    "- **Input**: 6 observations (exactly what our environment gives us)\n",
    "- **Hidden layers**: 128 and 64 neurons - enough to learn complex patterns but not too big to be slow\n",
    "- **Output**: 2 Q-values (one for each possible action)\n",
    "- **Activation**: ReLU (simple and effective for this type of problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d2796",
   "metadata": {},
   "source": [
    "## Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff2b3984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING THE NEURAL NETWORK ===\n",
      "🧠 DQN Network Created:\n",
      "   Input: 6 → Hidden: 128 → Hidden: 64 → Output: 2\n",
      "   Total parameters: 9,282\n",
      "Test observation: [0.5, 0.20000000298023224, 1.0, 0.800000011920929, 0.8999999761581421, 1.0]\n",
      "Network output (Q-values): [0.16926881670951843, 0.14115166664123535]\n",
      "Best action: 0 (wait)\n",
      "\n",
      "✅ Neural Network test completed!\n"
     ]
    }
   ],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network for Gravity Guy\n",
    "    \n",
    "    This neural network takes game observations and predicts Q-values for each action.\n",
    "    Think of it as the \"brain\" that learns to evaluate how good each action is.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=6, hidden_size1=128, hidden_size2=64, output_size=2):\n",
    "        \"\"\"\n",
    "        Initialize the network layers\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of observations (6 for our game)\n",
    "            hidden_size1: First hidden layer size (128 neurons)\n",
    "            hidden_size2: Second hidden layer size (64 neurons)  \n",
    "            output_size: Number of actions (2: wait or flip)\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # Define the network layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)      # Input → Hidden1\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)    # Hidden1 → Hidden2  \n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)     # Hidden2 → Output\n",
    "        \n",
    "        print(f\"🧠 DQN Network Created:\")\n",
    "        print(f\"   Input: {input_size} → Hidden: {hidden_size1} → Hidden: {hidden_size2} → Output: {output_size}\")\n",
    "        print(f\"   Total parameters: {sum(p.numel() for p in self.parameters()):,}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: convert observations into Q-values\n",
    "        \n",
    "        This is where the magic happens - observations go in, Q-values come out!\n",
    "        \n",
    "        Args:\n",
    "            x: Observations tensor [batch_size, 6]\n",
    "            \n",
    "        Returns:\n",
    "            Q-values tensor [batch_size, 2] - one Q-value for each action\n",
    "        \"\"\"\n",
    "        # Layer 1: observations → first hidden layer (with ReLU activation)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # Layer 2: first hidden → second hidden layer (with ReLU activation)  \n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Layer 3: second hidden → Q-values (no activation - we want raw Q-values)\n",
    "        q_values = self.fc3(x)\n",
    "        \n",
    "        return q_values\n",
    "\n",
    "# Test our network\n",
    "print(\"=== TESTING THE NEURAL NETWORK ===\")\n",
    "\n",
    "# Create the network\n",
    "dqn = DQN(input_size=6, output_size=2)\n",
    "\n",
    "# Test with a fake observation (like what our environment produces)\n",
    "test_observation = torch.tensor([0.5, 0.2, 1.0, 0.8, 0.9, 1.0], dtype=torch.float32)\n",
    "test_observation = test_observation.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Get Q-values from the network\n",
    "with torch.no_grad():  # No gradients needed for testing\n",
    "    q_values = dqn(test_observation)\n",
    "\n",
    "print(f\"Test observation: {test_observation.squeeze().tolist()}\")\n",
    "print(f\"Network output (Q-values): {q_values.squeeze().tolist()}\")\n",
    "print(f\"Best action: {torch.argmax(q_values).item()} ({'flip' if torch.argmax(q_values).item() == 1 else 'wait'})\")\n",
    "\n",
    "print(\"\\n✅ Neural Network test completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd35dacf",
   "metadata": {},
   "source": [
    "## Understanding the Network Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98665602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UNDERSTANDING NETWORK COMPONENTS ===\n",
      "Step-by-step forward pass:\n",
      "1. Input observations: [0.30000001192092896, -0.10000000149011612, -1.0, 0.6000000238418579, 0.699999988079071, 0.800000011920929]\n",
      "   Shape: torch.Size([1, 6])\n",
      "2. After first hidden layer (128 neurons): torch.Size([1, 128])\n",
      "   Sample values: [0.000, 0.179, 0.000, ...] (showing first 3)\n",
      "3. After second hidden layer (64 neurons): torch.Size([1, 64])\n",
      "   Sample values: [0.000, 0.000, 0.000, ...] (showing first 3)\n",
      "4. Final Q-values: [0.15515024960041046, 0.1052684634923935]\n",
      "   Q(wait) = 0.155, Q(flip) = 0.105\n",
      "5. Decision: Choose action 0 (wait)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== UNDERSTANDING NETWORK COMPONENTS ===\")\n",
    "\n",
    "# Let's examine what each layer does\n",
    "test_obs = torch.tensor([0.3, -0.1, -1.0, 0.6, 0.7, 0.8], dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "print(\"Step-by-step forward pass:\")\n",
    "print(f\"1. Input observations: {test_obs.squeeze().tolist()}\")\n",
    "\n",
    "# Manual forward pass to see each step\n",
    "x = test_obs\n",
    "print(f\"   Shape: {x.shape}\")\n",
    "\n",
    "# Layer 1\n",
    "x = F.relu(dqn.fc1(x))  \n",
    "print(f\"2. After first hidden layer (128 neurons): {x.shape}\")\n",
    "print(f\"   Sample values: [{x[0][0]:.3f}, {x[0][1]:.3f}, {x[0][2]:.3f}, ...] (showing first 3)\")\n",
    "\n",
    "# Layer 2  \n",
    "x = F.relu(dqn.fc2(x))\n",
    "print(f\"3. After second hidden layer (64 neurons): {x.shape}\")\n",
    "print(f\"   Sample values: [{x[0][0]:.3f}, {x[0][1]:.3f}, {x[0][2]:.3f}, ...] (showing first 3)\")\n",
    "\n",
    "# Layer 3\n",
    "x = dqn.fc3(x)\n",
    "print(f\"4. Final Q-values: {x.squeeze().tolist()}\")\n",
    "print(f\"   Q(wait) = {x[0][0]:.3f}, Q(flip) = {x[0][1]:.3f}\")\n",
    "\n",
    "# Decision making\n",
    "best_action = torch.argmax(x).item()\n",
    "print(f\"5. Decision: Choose action {best_action} ({'flip' if best_action == 1 else 'wait'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7140ff",
   "metadata": {},
   "source": [
    "## Key Concepts Explained\n",
    "\n",
    "**What just happened?**\n",
    "\n",
    "1. **Input Processing**: We fed the network 6 numbers representing the game state\n",
    "2. **Hidden Layers**: The network processed this information through two layers of neurons\n",
    "3. **Q-Value Output**: We got back 2 numbers - Q(wait) and Q(flip)  \n",
    "4. **Action Selection**: We pick the action with the highest Q-value\n",
    "\n",
    "**Why ReLU activation?**\n",
    "- ReLU (Rectified Linear Unit) simply makes negative values = 0\n",
    "- It's fast, simple, and works well for most problems\n",
    "- Helps the network learn complex patterns\n",
    "\n",
    "**Why no activation on the output?**\n",
    "- Q-values can be positive or negative (good or bad situations)  \n",
    "- We want the raw values, not constrained to 0-1 range"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
